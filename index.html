<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Jinfa Huang - Ph.D. Candidate at University of Rochester">
  <meta name="author" content="Jinfa Huang">
  <title>Jinfa Huang</title>
  
  <!-- ÁΩëÁ´ôÂõæÊ†á -->
  <link rel="icon" type="image/png" href="image/logo_hjf.png">
  
  <!-- ÂºïÂÖ• Google Fonts (Â≠¶ÊúØÂ∏∏Áî®Â≠ó‰Ωì) -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&family=Lato:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="stylesheet.css"> 
</head>

<body>

  <div class="container">

<!-- ================== 1. ‰∏™‰∫∫‰ø°ÊÅØ (Â±Ö‰∏≠Âπ≥Ë°°Áâà) ================== -->
    <div class="main-header-container">
        <span class="name-en-center">Jinfa Huang</span>
        <span class="name-cn-center">ÈªÑÈî¶Âèë</span>
    </div>

<!-- Á¨¨‰∫åÈÉ®ÂàÜÔºöÁÆÄ‰ªã‰∏éÁÖßÁâá (ÂèåÊ†è) -->
    <div class="profile-body">
      <div class="profile-bio">
        <p class="bio-text">
          ‚ú®Bonjour, I am a Ph.D. candidate in the Department of Computer Science, <a href="https://www.rochester.edu/">University of Rochester (UR)</a>, advised by <a href="https://www.cs.rochester.edu/u/jluo/">Prof. Jiebo Luo</a>.
        </p>
        <p class="bio-text">
          Prior to that, I got my master's degree from <a href="https://www.pku.edu.cn/">Peking University (PKU)</a>, advised by <a href="https://yuanli2333.github.io/">Prof. Li Yuan</a> and <a href="https://sites.google.com/view/jie-chen-2020/home">Prof. Jie Chen</a>. I obtained the honored bachelor's degree from <a href="https://en.uestc.edu.cn/">University of Electronic Science and Technology of China (UESTC)</a>.
        </p>

        <!-- ÁæéÂåñÂêéÁöÑÁ§æ‰∫§ÈìæÊé• -->
        <div class="social-links-bar">
          <a href="mailto:vhjf305@gmail.com" class="social-btn">Email</a>
          <a href="https://scholar.google.com/citations?user=2t7iBnkAAAAJ&hl=en&oi=ao" class="social-btn">Google Scholar</a>
          <a href="https://github.com/inFaaa" class="social-btn">Github</a>
          <a href="https://twitter.com/vhjf36495872" class="social-btn">Twitter</a>
          <a href="https://www.zhihu.com/people/vm2016" class="social-btn">Zhihu</a>
          <a href="https://www.linkedin.com/in/jinfa-huang-262a2929b/" class="social-btn">LinkedIn</a>
        </div>
      </div>

      <!-- ÁæéÂåñÂêéÁöÑÂ§¥ÂÉèÊ°Ü -->
      <div class="profile-photo-box">
        <img src="image/photo.jpg" alt="Jinfa Huang" class="avatar-img"
             onmouseover="this.src='image/photo5.jpg';" 
             onmouseout="this.src='image/photo.jpg';">
        <!-- Â¶ÇÊûúÊÉ≥‰øùÁïô‰∏ãÈù¢ÁöÑÊñáÂ≠óÔºåËß£Èô§Ê≥®ÈáäÂç≥ÂèØÔºåÂÆÉÁé∞Âú®‰ºöÂú®Ê°ÜÈáåÈù¢ -->
        <!-- <div style="margin-top:10px; font-size:0.85rem; color:#888; font-family:'Lato',sans-serif;">Winter 2024, Puerto Rico ‚ú®</div> -->
      </div>
    </div>

    <!-- Á¨¨‰∫åÈÉ®ÂàÜÔºöÊÑøÊôØ (ÂÖ®ÂÆΩ) -->
    <div class="vision-section-full">
      <p class="vision-intro">
        My long-term goal is to build multimodal, interactive AI systems that ground, reason, and generate within a closed loop. I conceptualize this pursuit as the <strong>Prometheus</strong> framework:
      </p>
      
      <div class="vision-cards">
        <div class="v-card">
          <div class="v-title">(1) Distilling the Spark</div>
          <div class="v-desc">Grounding autonomous intelligence in real-world environments via continuous human feedback.</div>
        </div>
        <div class="v-card">
          <div class="v-title">(2) Self-Evolving</div>
          <div class="v-desc">Aligning generative models with reward mechanisms to transition from passive recognition to understanding.</div>
        </div>
        <div class="v-card">
          <div class="v-title">(3) Agentic Autonomy</div>
          <div class="v-desc">Developing Agentic AI with planning, memory, and tool-use capabilities to autonomously learn and evolve.</div>
        </div>
      </div>
    </div>

    <!-- Á¨¨‰∏âÈÉ®ÂàÜÔºöResearch Map (Â±Ö‰∏≠Â§ßÂõæ) -->
    <div class="map-section-full">
      <img src="image/Research_Map.png" alt="Research Map" class="research-map-img">
      <p class="map-caption"><strong>Research Map:</strong> A schematic overview of my research vision.</p>
    </div>

    <!-- ================= Êñ∞Èóª (News) ================= -->
    <h2 class="section-title">News</h2>
    <ul class="news-list">
      <li><span class="news-date">[2025/11]</span><span class="news-content">1 paper (QuoTA) accepted by AAAI 2026.</span></li>
      <li><span class="news-date">[2025/10]</span><span class="news-content">I am excited to be selected as a volunteer at NeurIPS 2025! See you at San Diego üèñÔ∏è!</span></li>
      <li><span class="news-date">[2025/09]</span><span class="news-content">1 survey paper (Next-Gen AIGC) accepted by Frontiers of Computer Science 2025.</span></li>
      <li><span class="news-date">[2025/09]</span><span class="news-content">2 papers (OpenS2V-Nexus and VideoRAG) accepted by NeurIPS main and D&B track 2025.</span></li>
      
      <!-- ÈöêËóèÁöÑÊñ∞Èóª -->
      <div class="hidden-news" id="more-news">
        <li>
            <span class="news-date">[2025/08]</span>
            <span class="news-content">
                <a href="https://academicminute.org/jiebo-luo-university-of-rochester-text-to-video-ai-blossoms-with-new-metamorphic-video-capabilities/">"Text-to-Video AI Blossoms"</a> featured by The Academic Minute.
            </span>
        </li>
        <li><span class="news-date">[2025/08]</span><span class="news-content">1 paper (TACO) accepted by EMNLP Main 2025.</span></li>
        <li><span class="news-date">[2025/07]</span><span class="news-content">1 paper (MoE-LLaVA) accepted by TMM 2025.</span></li>
        <li>
            <span class="news-date">[2025/07]</span>
            <span class="news-content">
                Released <a href="https://github.com/multimodal-art-projection/LatentCoT-Horizon">LatentCoT-Horizon</a> repo for Survey on Latent Reasoning.
                <img alt="GitHub Stars" src="https://img.shields.io/github/stars/multimodal-art-projection/LatentCoT-Horizon?style=flat&logo=github&labelColor=white">
            </span>
        </li>
        <li><span class="news-date">[2025/06]</span><span class="news-content">Research career has achieved <a href="https://scholar.google.com/citations?user=2t7iBnkAAAAJ">1000+ Google Citations</a>! üöÄ</span></li>
        <li><span class="news-date">[2025/05]</span><span class="news-content">Workshop on <a href="https://mllm-mucg.github.io/MM2025/">MUCG</a> will be presented at ACMMM 2025.</span></li>
        <li><span class="news-date">[2025/05]</span><span class="news-content">MagicTime featured by <a href="https://www.rochester.edu/newscenter/ai-text-to-video-ai-metamorphic-capabilities-649992/">University of Rochester News</a>.</span></li>
        <li><span class="news-date">[2025/04]</span><span class="news-content">Passed qualification exam, officially a Ph.D. Candidate! üéâ</span></li>
        <li><span class="news-date">[2025/04]</span><span class="news-content">MagicTime accepted by <strong>TPAMI 2025</strong>.</span></li>
        <li><span class="news-date">[2025/03]</span><span class="news-content">AR-Visual Survey accepted by TMLR 2025.</span></li>
        <li><span class="news-date">[2025/02]</span><span class="news-content">ConsisID accepted by <strong>CVPR 2025 Highlight</strong> (Top 3%).</span></li>
        <li><span class="news-date">[2025/01]</span><span class="news-content">2 papers (1 Poster, 1 Spotlight) accepted by ICLR 2025.</span></li>
        <li><span class="news-date">[2025/01]</span><span class="news-content">Medical LLM Survey accepted by <strong>Nature Reviews Bioengineering 2025</strong>.</span></li>
        <li><span class="news-date">[2025/01]</span><span class="news-content">Started research internship at Google, USA.</span></li>
        <li><span class="news-date">[2024/12]</span><span class="news-content">PromptLLM accepted by TPAMI 2025.</span></li>
        <li><span class="news-date">[2024/12]</span><span class="news-content">1 paper accepted by AAAI 2025.</span></li>
        <li><span class="news-date">[2024/12]</span><span class="news-content">1 short paper accepted by COLING 2025.</span></li>
        <li><span class="news-date">[2024/11]</span><span class="news-content">1 paper accepted by TIST 2024.</span></li>
        <li><span class="news-date">[2024/11]</span><span class="news-content">1 paper accepted by npj Digital Medicine.</span></li>
        <li><span class="news-date">[2024/11]</span><span class="news-content">1 survey accepted by CAAI Transactions on Intelligence Technology.</span></li>
        <li><span class="news-date">[2024/10]</span><span class="news-content">Released <a href="https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey">Autoregressive Models in Vision Survey</a>.</span></li>
        <li><span class="news-date">[2024/09]</span><span class="news-content">1 paper (Spotlight) accepted by NeurIPS 2024 Datasets & Benchmarks Track.</span></li>
        <li><span class="news-date">[2024/09]</span><span class="news-content">1 paper accepted by EMNLP 2024 Findings.</span></li>
        <li><span class="news-date">[2024/06]</span><span class="news-content">Released <a href="https://pku-yuangroup.github.io/ChronoMagic-Bench/">ChronoMagic-Bench</a>.</span></li>
        <li><span class="news-date">[2024/05]</span><span class="news-content">Started research internship at ByteDance Seed, USA.</span></li>
        <li><span class="news-date">[2024/05]</span><span class="news-content">1 paper accepted by ACL 2024 Findings.</span></li>
        <li><span class="news-date">[2024/04]</span><span class="news-content">Released <a href="https://pku-yuangroup.github.io/MagicTime">MagicTime</a>.</span></li>
        <li><span class="news-date">[2024/01]</span><span class="news-content">1 paper accepted by ICLR 2024.</span></li>
        <li><span class="news-date">[2023/11]</span><span class="news-content">Released <a href="https://github.com/AI-in-Health/MedLLMsPracticalGuide">MedLLMsPracticalGuide</a>.</span></li>
        <li><span class="news-date">[2023/11]</span><span class="news-content">Released <a href="https://arxiv.org/abs/2311.07547">GPT-4V(ision) Analysis</a>.</span></li>
        <li><span class="news-date">[2023/09]</span><span class="news-content">Join the VIStA Lab as a Ph.D. student.</span></li>
        <li><span class="news-date">[2023/07]</span><span class="news-content">1 paper accepted by ACMMM 2023.</span></li>
        <li><span class="news-date">[2023/05]</span><span class="news-content">Awarded 2023 Peking University Excellent Graduation Thesis.</span></li>
        <li><span class="news-date">[2023/04]</span><span class="news-content">1 paper accepted by TIP 2023.</span></li>
        <li><span class="news-date">[2023/04]</span><span class="news-content">1 paper accepted by IJCAI 2023.</span></li>
        <li><span class="news-date">[2023/02]</span><span class="news-content">1 paper (Top 10% Highlight) accepted by CVPR 2023.</span></li>
        <li><span class="news-date">[2022/09]</span><span class="news-content">1 paper accepted by ICRA 2023.</span></li>
        <li><span class="news-date">[2022/09]</span><span class="news-content">1 paper (Spotlight) accepted by NeurIPS 2022.</span></li>
      </div>
    </ul>
    <button class="btn-show-more" onclick="toggleNews()" id="news-toggle-btn">Show More</button>

    <!-- ================= ÊïôËÇ≤ËÉåÊôØ (Education) ================= -->
    <h2 class="section-title">Education</h2>
    
    <div class="experience-item">
      <div class="logo-box"><img src='image/UR_logo.webp' alt="UR Logo"></div>
      <div class="experience-details">
        <strong>University of Rochester (UR), USA</strong>
        <span>Ph.D. Student in Computer Science &bull; Sep. 2023 - Present</span><br>
        <em>Advisor: <a href="https://www.cs.rochester.edu/u/jluo/">Prof. Jiebo Luo</a></em>
      </div>
    </div>

    <div class="experience-item">
      <div class="logo-box"><img src='image/pku_logo.png' alt="PKU Logo"></div>
      <div class="experience-details">
        <strong>Peking University (PKU), China</strong>
        <span>Master Degree in Computer Science &bull; Sep. 2020 - Jun. 2023</span><br>
        <em>Advisors: <a href="https://yuanli2333.github.io/">Prof. Li Yuan</a> & <a href="https://sites.google.com/view/jie-chen-2020/home">Prof. Jie Chen</a></em>
      </div>
    </div>

    <div class="experience-item">
      <div class="logo-box"><img src='image/uestc_icon1.png' alt="UESTC Logo"></div>
      <div class="experience-details">
        <strong>University of Electronic Science and Technology of China (UESTC)</strong>
        <span>Bachelor Degree in Software Engineering &bull; Sep. 2016 - Jun. 2020</span><br>
        <em>Advisors: <a href="https://ieeexplore.ieee.org/author/37288619300">Xucheng Luo</a> </em>
      </div>
    </div>

<!-- ================== Research Experience (Â∏¶ÂØºÂ∏àÈìæÊé•Áâà) ================== -->
    <h2 class="section-title">Selected Research Experience</h2>

    <div class="experience-item">
      <div class="logo-box">
        <a href="https://research.google/">
             <!-- Google Logo (PNG) -->
            <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Google_2015_logo.svg/1200px-Google_2015_logo.svg.png" alt="Google">
        </a>
      </div>
      <div class="experience-details">
        <strong>Google Research, USA</strong>
        <span>Student Researcher &bull; Sep. 2025 - Present</span><br>
        <em>Advisor: <a href="https://scholar.google.com/citations?user=xrND8B8AAAAJ&hl=en">Dr. Junfeng He</a></em>
      </div>
    </div>

    <div class="experience-item">
      <div class="logo-box">
        <a href="https://www.aboutamazon.com/">
            <img src='image/amazon.png' alt="Amazon">
        </a>
      </div>
      <div class="experience-details">
        <strong>International Machine Learning, Amazon, USA</strong>
        <span>Applied Scientist Intern &bull; Jul. 2025 - Sep. 2025</span><br>
        <em>Advisors: <a href="https://scholar.google.com/citations?user=j6cZk58AAAAJ&hl=en">Dr. Yang Liu</a>, <a href="https://scholar.google.com/citations?user=0Uc6o6EAAAAJ&hl=en">Dr. Chien-Chih Wang</a>, <a href="https://harryliew.github.io/">Dr. Huidong Liu</a></em>
      </div>
    </div>

    <div class="experience-item">
      <div class="logo-box">
        <a href="https://research.google/">
            <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Google_2015_logo.svg/1200px-Google_2015_logo.svg.png" alt="Google">
        </a>
      </div>
      <div class="experience-details">
        <strong>Core ML Applied ML, Google, USA</strong>
        <span>Student Researcher &bull; Jan. 2025 - May. 2025</span><br>
        <em>Advisors: <a href="https://scholar.google.com/citations?user=umEBJkQAAAAJ&hl=en">Jiageng Zhang</a>, <a href="https://scholar.google.com/citations?user=Nun8Dy0AAAAJ&hl=en&authuser=1">Dr. Eric Li</a></em>
      </div>
    </div>

    <div class="experience-item">
      <div class="logo-box">
        <a href="https://www.bytedance.com/en/">
            <img src='image/bytedance_logo.png' alt="ByteDance">
        </a>
      </div>
      <div class="experience-details">
        <strong>Seed Foundation Model, ByteDance, USA</strong>
        <span>Research Intern &bull; May 2024 - Aug. 2024</span><br>
        <em>Advisors: <a href="https://qzyou.github.io/">Dr. Quanzeng You</a>, <a href="https://sites.google.com/view/yongfei-liu/">Dr. Yongfei Liu</a>, <a href="https://scholar.google.com/citations?user=B1EhbCsAAAAJ&hl=en">Dr. Jianbo Yuan</a></em>
      </div>
    </div>

    <!-- ================= ËÆ∫Êñá (Publications) - ÊÅ¢Â§çÈ´ò‰∫ÆÂíåÂõæÊ†á ================= -->
    <h2 class="section-title">Selected Publications</h2>
    <p>My current research mainly focuses on vision+language and generative models. (*Equal Contribution)</p>

    <!-- Paper 1 -->
    <div class="paper-item">
      <div class="paper-img">
        <img src='project/MagicTime_Arxiv2024.png' alt="MagicTime">
      </div>
      <div class="paper-text">
        <a href="https://pku-yuangroup.github.io/MagicTime/" class="paper-title">MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators</a>
        <span class="paper-authors">
          Shenghai Yuan*, <strong>Jinfa Huang*</strong>, Yujun Shi, Yongqi Xu, Ruijie Zhu, Bin Lin, Xinhua Cheng, Li Yuan, Jiebo Luo
        </span>
        <span class="paper-venue">
          <span class="highlight-venue">TPAMI 2025</span> (IEEE Transactions on Pattern Analysis and Machine Intelligence)
        </span>
        <div class="paper-links">
          <a href="https://arxiv.org/abs/2404.05014">[Paper]</a>
          <a href="https://github.com/PKU-YuanGroup/MagicTime">[Code]</a>
          <a href="https://pku-yuangroup.github.io/MagicTime/">[Page]</a>
          <a href="https://www.rochester.edu/newscenter/ai-text-to-video-ai-metamorphic-capabilities-649992/">[News]</a>
          <img alt="Stars" src="https://img.shields.io/github/stars/PKU-YuanGroup/MagicTime?style=social">
        </div>
        <p class="paper-desc">
           We propose MagicTime, a metamorphic time-lapse video generation model, which learns real-world physics knowledge from time-lapse videos and implements metamorphic video generation.
        </p>
      </div>
    </div>

    <!-- Paper 2 -->
    <div class="paper-item">
      <div class="paper-img">
        <img src='project/HBI_CVPR2023.png' alt="HBI">
      </div>
      <div class="paper-text">
        <a href="https://arxiv.org/abs/2303.14369" class="paper-title">Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning</a>
        <span class="paper-authors">
          <a href="https://jpthu17.github.io/">Peng Jin</a>, <strong>Jinfa Huang</strong>, Pengfei Xiong, Shangxuan Tian, Chang Liu, Xiangyang Ji, Li Yuan, Jie Chen
        </span>
        <span class="paper-venue">
          <span class="highlight-venue">CVPR 2023</span> (Highlight, Top 2.5%)
        </span>
        <div class="paper-links">
          <a href="https://arxiv.org/abs/2303.14369">[Paper]</a>
          <a href="https://github.com/jpthu17/HBI">[Code]</a>
          <img alt="Stars" src="https://img.shields.io/github/stars/jpthu17/HBI?style=social">
        </div>
        <p class="paper-desc">
          We propose Expectation-Maximization Contrastive Learning (EMCL) to learn compact video-and-language representations using Hierarchical Banzhaf Interaction.
        </p>
      </div>
    </div>

    <!-- Paper 3 -->
    <div class="paper-item">
      <div class="paper-img">
        <img src='project/EMCL_NIPS2022.png' alt="EMCL">
      </div>
      <div class="paper-text">
        <a href="https://arxiv.org/abs/2211.11427" class="paper-title">Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations</a>
        <span class="paper-authors">
          Peng Jin*, <strong>Jinfa Huang*</strong>, Fenglin Liu, Xian Wu, Shen Ge, Guoli Song, David A. Clifton, Jie Chen
        </span>
        <span class="paper-venue">
          <span class="highlight-venue">NeurIPS 2022</span> (Spotlight, Top 5%)
        </span>
        <div class="paper-links">
          <a href="https://arxiv.org/abs/2211.11427">[Paper]</a>
          <a href="https://github.com/jpthu17/EMCL">[Code]</a>
          <img alt="Stars" src="https://img.shields.io/github/stars/jpthu17/EMCL?style=social">
        </div>
      </div>
    </div>


    <!-- ================= ÁªºËø∞‰∏é Benchmark (Lists) ================= -->
    <h2 class="section-title">Selected Surveys</h2>
    <p>I maintain several repositories to track the latest research.</p>
    <ul class="custom-list">
      <li>
        <span class="tag-label">[Arxiv 2025]</span> 
        <a href="https://github.com/inFaaa/Awesome-Personalized-Video-Creation"><strong>Personalized Video Generation: Progress, Applications, and Challenges</strong></a>
        <img alt="Stars" src="https://img.shields.io/github/stars/inFaaa/Awesome-Personalized-Video-Creation?style=flat&logo=github&labelColor=white" style="vertical-align:middle; margin-left:8px;">
      </li>
      <li>
        <span class="tag-label">[Arxiv 2025]</span> 
        <a href="https://github.com/multimodal-art-projection/LatentCoT-Horizon"><strong>A Survey on Latent Reasoning</strong></a>
        <img alt="Stars" src="https://img.shields.io/github/stars/multimodal-art-projection/LatentCoT-Horizon?style=flat&logo=github&labelColor=white" style="vertical-align:middle; margin-left:8px;">
      </li>
      <li>
        <span class="tag-label">[TMLR 2025]</span> 
        <a href="https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey"><strong>Autoregressive Models in Vision Survey</strong></a>
        <img alt="Stars" src="https://img.shields.io/github/stars/ChaofanTao/Autoregressive-Models-in-Vision-Survey?style=flat&logo=github&labelColor=white" style="vertical-align:middle; margin-left:8px;">
      </li>
      <li>
        <span class="tag-label">[NNature Reviews Bioengineering 2025]</span> 
        <a href="https://github.com/AI-in-Health/MedLLMsPracticalGuide"><strong>A Practical Guide for Medical Large Language Models</strong></a>
        <img alt="Stars" src="https://img.shields.io/github/stars/AI-in-Health/MedLLMsPracticalGuide?style=flat&logo=github&labelColor=white" style="vertical-align:middle; margin-left:8px;">
      </li>
    </ul>

    <h2 class="section-title">Selected Benchmarks</h2>
    <ul class="custom-list">
      <li>
        <span class="tag-label">[Arxiv 2025]</span> 
        <a href="https://github.com/PKU-YuanGroup/OpenS2V-Nexus"><strong>OpenS2V-Nexus: A Detailed Benchmark for Subject-to-Video Generation</strong></a>
        <img alt="Stars" src="https://img.shields.io/github/stars/PKU-YuanGroup/OpenS2V-Nexus?style=flat&logo=github&labelColor=white" style="vertical-align:middle; margin-left:8px;">
      </li>
      <li>
        <span class="tag-label">[NeurIPS 2024]</span> 
        <a href="https://github.com/PKU-YuanGroup/ChronoMagic-Bench"><strong>ChronoMagic-Bench: Metamorphic Evaluation of Text-to-Time-lapse Video</strong></a>
        <img alt="Stars" src="https://img.shields.io/github/stars/PKU-YuanGroup/ChronoMagic-Bench?style=flat&logo=github&labelColor=white" style="vertical-align:middle; margin-left:8px;">
      </li>
      <li>
        <span class="tag-label">[TIST 2025]</span> 
        <a href="https://github.com/VIStA-H/GPT-4V%5fSocial%5fMedia"><strong>GPT-4V(ision) as A Social Media Analysis Engine</strong></a>
        <img alt="Stars" src="https://img.shields.io/github/stars/VIStA-H/GPT-4V%5fSocial%5fMedia?style=flat&logo=github&labelColor=white" style="vertical-align:middle; margin-left:8px;">
      </li>
    </ul>


    <!-- ================== 9. Misc (Invited Talks & Service) ================== -->
    
    <!-- Invited Talks Section (Full Width) -->
    <h2 class="section-title">Invited Talks</h2>
    <ul class="custom-list">
        <li>"OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation‚Äú, University of Rochester, GIDS-10th Anniversary, 2025.09, [<a href="https://www.hajim.rochester.edu/dsc/anniversary.html">Poster</a>] </li>
        <li>"MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators", NYC Computer Vision Day, 2025.02, [<a href="https://cs.nyu.edu/~fouhey/NYCVision2025/">Poster</a>] </li>
        <li>"Can Video Generation Models as World Simulators?‚Äú, 3DËßÜËßâÂ∑•Âùä, 2025.01, [<a href="https://mp.weixin.qq.com/s/-5lmd0ZRw1IKHFgom_sbzg">Live</a>] </li>
        <li>"Text-to-video AI blossoms with New Metamorphic Video Capabilities‚Äú, University of Rochester News, 2025.05, [<a href="https://www.rochester.edu/newscenter/ai-text-to-video-ai-metamorphic-capabilities-649992/">News</a>] </li>
    </ul>
    
    <!-- Bilibili Video Container (Restored) -->
    <div class="video-container">
        <iframe src="//player.bilibili.com/player.html?isOutside=true&aid=113802641150520&bvid=BV1pHc7e1E2u&cid=27792247451&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>
    </div>

<!-- ================== 9. Service, Teaching & Life (ÁæéÂåñÊéíÁâà) ================== -->
    
    <!-- Á¨¨‰∏ÄË°åÔºöÂ≠¶ÊúØÊúçÂä°ÂíåÊïôÂ≠¶ (ÂèåÊ†èÂ∏ÉÂ±Ä) -->
    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 50px; margin-top: 70px;">
        <div>
            <h2 class="section-title" style="margin-top: 0;">Academic Service</h2>
            <ul class="custom-list">
              <li><strong>Program Member:</strong> MUCG@ACMMM2025, ER@NeurIPS2025</li>
              <li><strong>PC Member:</strong> CVPR'23-26, NeurIPS'22-25, ICLR'23-26, ICCV'23/25, ACM MM'24/25, ECCV'24, AAAI'25/26, COLM'25, ACL'25/26</li>
              <li><strong>Journal Reviewer:</strong> IEEE TPAMI(x2), IJCV, IEEE TCSVT, NEJM AI</li>
              <li><strong>Volunteer:</strong> NeurIPS 2025</li>
            </ul>
        </div>
        
        <div>
            <h2 class="section-title" style="margin-top: 0;">Teaching</h2>
            <ul class="custom-list">
                <li><strong>TA, CSC 240/440 Data Mining</strong>, <a href="https://www.cs.rochester.edu/people/faculty/polak_monika/index.html">Prof. Monika Polak</a>, UR, 2025 Fall</li>
                <li><strong>TA, CSC 240/440 Data Mining</strong>, <a href="https://www.cs.rochester.edu/u/pawlicki/">Prof. Thaddeus E. Pawlicki</a>, UR, 2025 Spring</li>
                <li><strong>TA, CSC 240/440 Data Mining</strong>, <a href="https://www.cs.rochester.edu/people/faculty/polak_monika/index.html">Prof. Monika Polak</a>, UR, 2024 Fall</li>
            </ul>
        </div>
    </div>
    
    <!-- Á¨¨‰∫åË°åÔºö‰∏™‰∫∫ÂÖ¥Ë∂£‰∏éÁîüÊ¥ªÁÖß (Â∑¶ÊñáÂè≥ÂõæÔºåÊùÇÂøóÈ£éÊ†º) -->
    <h2 class="section-title">Personal Interests</h2>
    
    <div class="interests-container">
        <!-- Â∑¶‰æßÔºöÊñáÂ≠ó -->
        <div class="interests-text">
            <ul class="custom-list">
              <li><strong>Anime:</strong> Love, sports, and sci-fi.</li>
              <li><strong>Literature:</strong> <a href="https://en.wikipedia.org/wiki/Wang_Xiaobo">Xiaobo Wang</a> & <a href="https://en.wikipedia.org/wiki/Friedrich_Nietzsche">Friedrich Nietzsche</a>.</li>
              <li><strong>Coding:</strong> Maintainer of <a href="https://github.com/inFaaa/PKU-Beamer-Theme">PKU-Beamer-Theme</a> and <a href="https://github.com/inFaaa/build-your-own-x-vibe-coding">Build-Your-Own-X-with-Vibe-Coding</a>.</li>
            </ul>
        </div>

        <!-- Âè≥‰æßÔºöÂõæÁâá (Â∏¶Á≤æËá¥ÁöÑÂç°ÁâáÊïàÊûú) -->
        <div class="interests-photo">
            <img src="image/rushrushrush.jpg" alt="Life" class="life-img">
            <!-- <div class="img-caption">Rush Rush Rush üèÉ‚Äç‚ôÇÔ∏è</div> -->
        </div>
    </div>

    <!-- ================= È°µËÑö‰∏éÂú∞Âõæ ================= -->
    <div class="footer">
      <div id="clustrmaps-container" class="map-container">
        <!-- ËøòÂéüÂéüÂßãÂú∞Âõæ‰ª£Á†Å -->
        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=g8JfCuglsVHk6L-tAQR2jVnplj3XNoA-rUSHfEZOf8M&cl=ffffff&w=a"></script>
      </div>
      
      <p>
        My hometown is Guangdong, you can call me by my Cantonese name: Gamfaat Wong.<br>
        Last updated on Nov 2025.
      </p>
      <p style="font-size: 0.8em; color: #999;">
        Template inspired by <a href="https://people.eecs.berkeley.edu/~barron/">Jon Barron</a>.
      </p>
    </div>

  </div>

  <!-- ================= ËÑöÊú¨ ================= -->
  <script>
    function toggleNews() {
      var hiddenNews = document.getElementById('more-news');
      var btn = document.getElementById('news-toggle-btn');
      
      if (hiddenNews.style.display === 'block') {
        hiddenNews.style.display = 'none';
        btn.textContent = 'Show More';
      } else {
        hiddenNews.style.display = 'block';
        btn.textContent = 'Show Less';
      }
    }
  </script>

</body>
</html>