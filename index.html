<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Jinfa Huang</title>
  
  <meta name="author" content="Jinfa Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="image/logo_hjf.png">
  
</head>
 
  <body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Jinfa Huang &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		        <img style="vertical-align:middle" src='image/vhjf_name.png' height='90px' width='WIDTHpx'>
		        </name>
        </p>
		<p>
      <p>Currently, I am a first-year Ph.D. student in the Department of Computer Science, <a href="https://www.rochester.edu/">University of Rochester (UR)</a>, advised by <a href="https://www.cs.rochester.edu/u/jluo/">Prof. Jiebo Luo</a>. </p>
      <p>I aim at building multimodal interactive AI systems that can not only ground and reason over the external world signals, to understand human language, but also assist humans in decision-making and efficiently solving social concerns, e.g., robot. 
        As steps towards this goal, my research interests include but are not limited to <strong>Multimodal Large Language Models</strong>, <strong>Video Language Alignment</strong> and  <strong>Multimodal Agents</strong>.
      </p>
        Prior to that, I got my master's degree from <a href="https://www.pku.edu.cn/">Peking University (PKU)</a> in 2023, advised by <a href="https://yuanli2333.github.io/">Prof. Li Yuan</a> and <a href="https://sites.google.com/view/jie-chen-2020/home">Prof. Jie Chen</a>. 
    And I obtained the honored bachelor's degree from <a href="https://en.uestc.edu.cn/">University of Electronic Science and Technology of China (UESTC)</a> in 2020.  
    </p>


        <p align=center>
          <a href="mailto:vhjf305@gmail.com">Email</a> &nbsp/&nbsp
	        <a href="https://scholar.google.com/citations?user=2t7iBnkAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
          <a href="https://github.com/inFaaa">Github</a> 
        </p>

        </td>
        <td width="33%">
        <img src="image/photo.jpg" width="300">
        </td>
      </tr>
  </table> 


<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
        <td width="100%" valign="middle">
            <heading>News</heading>
            <p>
                <li> <strongsmall>[2024/03]</strongsmall> &nbsp;&nbsp;<smalll> I will be joining ByteDance AML, Seattle, USA, as a research intern this summer.</smalll><br/>
                <li> <strongsmall>[2024/01]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by ICLR 2024.</smalll><br/>
                <li> <strongsmall>[2023/11]</strongsmall> &nbsp;&nbsp;<smalll>ðŸ”¥ðŸ”¥ðŸ”¥ We release a GitHub repository to promote medical Large Language Models research with the vision of applying LLM to real-life medical scenarios: <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/AI-in-Health/MedLLMsPracticalGuide?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/AI-in-Health/MedLLMsPracticalGuide">A Practical Guide for Medical Large Language Models. </a></smalll><br/> 
                <li> <strongsmall>[2023/11]</strongsmall> &nbsp;&nbsp;<smalll>ðŸ”¥ðŸ”¥ðŸ”¥ How could LMMs contribute to social good? We are excited to release a new preliminary explorations of GPT-4V(ison) for social multimedia: <a href="https://arxiv.org/abs/2311.07547">GPT-4V(ision) as A Social Media Analysis Engine.</a> </smalll><br/>          
                <li class="hidden-news"><strongsmall>[2023/09]</strongsmall> &nbsp;&nbsp;<smalll>Join the  <a href="https://www.cs.rochester.edu/u/jluo/#Prospective">VIStA Lab</a> as a Ph.D. student working on vision and language.</smalll><br/>
                <li class="hidden-news"><strongsmall> [2023/07]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by ACMMM 2023.</smalll><br/>
                <li class="hidden-news"><strongsmall>[2023/05]</strongsmall> &nbsp;&nbsp;<smalll>I was awarded the 2023 Peking University Excellent Graduation Thesis.</smalll><br/>
                <li class="hidden-news"><strongsmall>[2023/04]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by TIP 2023.</smalll><br/>
                <li class="hidden-news"><strongsmall>[2023/04]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by IJCAI 2023.</smalll><br/>
                <li class="hidden-news"><strongsmall>[2023/02]</strongsmall> &nbsp;&nbsp;<smalll>1 paper (Top 10% Highlight) is accepted by CVPR 2023.</smalll><br/>
                <li class="hidden-news"> <strongsmall>[2022/09]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by ICRA 2023.</smalll><br/>
                <li class="hidden-news"> <strongsmall>[2022/09]</strongsmall> &nbsp;&nbsp;<smalll>1 paper (Spotlight) is accepted by NeurIPS 2022.</smalll><br/> 
            </p>
        </td>
    </tr>
</table>

<button onclick="toggleNews()">Show More</button>

<script>
function toggleNews() {
    var hiddenNews = document.querySelectorAll('.hidden-news');

    hiddenNews.forEach(function(news) {
        if (news.style.display === 'none' || news.style.display === '') {
            news.style.display = 'list-item';
        } else {
            news.style.display = 'none';
        }
    });
}
</script>



<p></p><p></p><p></p><p></p><p></p>
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Education</heading>
        </td>
      </tr>
      </table>
  
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
      <td width="10%">
        <img src='image/UR_logo.webp' width="100">
      </td>

      <td width="75%" valign="middle">
      <p>
      <stronghuge>University of Rochester (UR), USA</stronghuge><br />
      PH.D. Student in Computer Science &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2023 - Present <br />
      Advisor: <a href="https://www.cs.rochester.edu/u/jluo/">Prof. Jiebo Luo</a>
      </p>
    </td>
  </tr>

  <tr>
      <td width="10%">
        <img src='image/pku_logo.png' width="100">
      </td>

      <td width="75%" valign="middle">
      <p>
      <stronghuge>Peking University (PKU), China</stronghuge><br />
      Master Degree in Computer Science &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2020 - Jun. 2023 <br />
      Advisors:  <a href="https://yuanli2333.github.io/">Prof. Li Yuan</a> and <a href="https://sites.google.com/view/jie-chen-2020/home">Prof. Jie Chen</a>
      </p>
    </td>
  </tr>

    <tr>
        <td width="10%">
          <img src='image/uestc_icon1.png' width="100">
        </td>

        <td width="75%" valign="middle">
        <p>
        <stronghuge>University of Electronic Science and Technology of China (UESTC), China</stronghuge><br />
        Bachelor Degree in Software Engineering &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2016 - Jun. 2020 <br />
        </p>
      </td>
    </tr>
  </table>



<p></p><p></p><p></p><p></p><p></p>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Selected Publication <a href="https://scholar.google.com/citations?hl=en&user=2t7iBnkAAAAJ" style="font-size:22px;">[Google Scholar]</a></heading>
          <p>
            My current research mainly focues on vision+language. *Equal Contribution.
            <!-- I've also worked on human-centered visual understanding, such as human action recognition and parsing. -->
            Representative works are <span class="highlight">highlighted</span>.
            </p>
        </td>
      </tr>
      </table>
  
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" bgcolor="#ffffd0">
    <td width="15%">
      <img src='project/PQDiff_ICLR2024.png', width="200" height="150">
    </td>
    <td valign="top" width="100%">
  <strong>Continuous-Multiple Image Outpainting in One-Step via Positional Query and A Diffusion-based Approach</strong><br>
  <a href="https://sherrylone.github.io/">Shaofeng Zhang</a>,
  <strong>Jinfa Huang</strong>,
  <a>Qiang Zhou</a>,
  <a>Zhibin Wang</a>,    
  <a>Fan Wang</a>,
  <a>Jiebo Luo</a>,
  <a>Junchi Yan</a>
  <br>
      <em>Conference on International Conference on Learning Representations, <strong>ICLR 2024</strong></em><br>
      <em><strong><font color="#a82e2e">(Review Rating: 866)</font></strong></em> <br>
  <a href="https://openreview.net/forum?id=7hxoYxKDTV">[Paperlink]</a>, <a href="https://github.com/Sherrylone/PQDiff">[Code]</a><br>
      <em><strong>Area:</strong> Image Outpainting, Diffusion Model, GenAI</em> <br>
      <p></p>
  <p> We have proposed PQDiff, which learns the positional relationships and pixel information at the
    same time. Methodically, PQDiff can outpaint at any multiple in only one step, greatly increasing the
    applicability of image outpainting.</p>
    </td>
  </tr>
        </table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" bgcolor="#ffffd0">
    <td width="15%">
      <img src='project/HBI_CVPR2023.png', width="200" height="150">
    </td>
    <td valign="top" width="100%">
  <strong>Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning</strong><br>
  <a href="https://jpthu17.github.io/">Peng Jin</a>,
  <strong>Jinfa Huang</strong>,
  <a>Pengfei Xiong</a>,
  <a>Shangxuan Tian</a>,    
  <a>Chang Liu</a>,
  <a>Xiangyang Ji</a>,
  <a>Li Yuan</a>,
  <a>Jie Chen</a>
  <br>  
      <em>IEEE International Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2023</strong></em><br>
      <em><strong><font color="#a82e2e">(Highlight, Top 2.5%)</font></strong></em> <br>
  <a href="https://arxiv.org/abs/2211.11427">[Paperlink]</a>, <a href="https://github.com/jpthu17/EMCL">[Code]</a><br>
  <em><strong>Area:</strong> Video-and-Language Representation, Machine Learning, Video-Text Retrieval, Video Captioning</em> <br>
      <p></p>
  <p>To solve the problem of modality gap in video-text feature space, we propose Expectation-Maximization Contrastive Learning (EMCL) to learn compact video-and-language representations. 
    We use the Expectation-Maximization algorithm to find a compact set of bases for the latent space, where the features could be concisely represented as the linear combinations of these bases.</p>
    </td>
    </td>
  </tr>
       </table>
  
 
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
<tr onmouseout="aperture_stop()" onmouseover="aperture_start()" bgcolor="#ffffd0">
  <td width="15%">
    <img src='project/EMCL_NIPS2022.png', width="200" height="150">
  </td>
  <td valign="top" width="100%">
<strong>Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations</strong><br>
<a href="https://jpthu17.github.io/">Peng Jin*</a>,
<strong>Jinfa Huang*</strong>,
<a>Fenglin Liu</a>,
<a>Xian Wu</a>,    
<a>Shen Ge</a>,
<a>Guoli Song</a>,
<a>David A. Clifton</a>,
<a>Jie Chen</a>
<br>
    <em>Conference on Neural Information Processing Systems, <strong>NeurIPS 2022</strong></em><br>
<em><strong><font color="#a82e2e">(Spotlight Presentation, Top 5%)</font></strong></em> <br>

<a href="https://arxiv.org/abs/2211.11427">[Paperlink]</a>, <a href="https://github.com/jpthu17/EMCL">[Code]</a><br>
    <em><strong>Area:</strong> Video-and-Language Representation, Machine Learning, Video-Text Retrieval, Video Captioning</em> <br>
    <p></p>
<p>To solve the problem of the modality gap in video-text feature space, we propose Expectation-Maximization Contrastive Learning (EMCL) to learn compact video-and-language representations. We use the Expectation-Maximization algorithm to find a compact set of bases for the latent space, where the features could be concisely represented as the linear combinations of these bases.</p>
  </td>
</tr>
      </table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="15%">
      <img src='project/3D_TIP2023.png', width="200" height="150">
    </td>
    <td valign="top" width="100%">
  <strong>Weakly-Supervised 3D Spatial Reasoning for Text-based Visual Question Answering</strong><br>
  <a>Hao Li</a>,
  <strong>Jinfa Huang</strong>,
  <a>Peng Jin</a>,
  <a>Guoli Song</a>,
  <a>Qi Wu</a>,
  <a>Jie Chen</a>     
  <br>
      <em>IEEE Transactions on Image Processing, <strong>TIP 2023</strong></em><br>
      <a href="https://ieeexplore.ieee.org/document/10141570">[Paperlink]</a><br>
      <em><strong>Area:</strong> 3D Spatial Reasoning, Text-based Visual Question Answering</em> <br>
      <p></p>
  <p>Existing approaches are constrained within 2D spatial information learned from the input images and rely on transformer-based architectures to reason implicitly during the fusion process. 
    spatial reasoning between texts and objects is crucial in TextVQA, we introduce 3D geometric information into a human-like spatial reasoning process to capture the contextual knowledge of key objects step-by-step. </p>
    </td>
  </tr>
        </table>

<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="15%">
      <img src='project/TR_ICRA2023.png', width="200" height="150">
    </td>
    <td valign="top" width="75%">
  <strong>Cross-Modality Time-Variant Relation Learning for Generating Dynamic Scene Graphs</strong><br>
  <a href="https://arxiv.org/pdf/2305.08522.pdf">[Paperlink]</a> , <a href="https://github.com/qncsn2016/TR2">[Code]</a><br>
  <a>Jingyi Wang</a>,
  <strong>Jinfa Huang</strong>,
  <a>Can Zhang</a>,
  <a>Zhidong Deng</a>    
  <br>
      <em>Proceedings of the International Conference on Robotics and Automation, <strong>ICRA 2023</strong></em><br>
      <em>Area: Cross-Modal Representation, Scene Graph Generation</em> <br>
      <p></p>
  <p>In the process of temporal and spatial modeling during dynamic scene graph generation, it is particularly intractable to learn time-variant relations in dynamic scene graphs among frames. 
    In this paper, we propose a Time-variant Relation-aware TRansformer, which aims to model the temporal change of relations in dynamic scene graphs. </p>
    </td>
  </tr>
        </table> -->

<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Selected Honors & Scholarships</heading>
          <div style="line-height:25px">
          <p>
		  <li> <stronghuge>Outstanding Graduate of University of Electronic Science and Technology of China (UESTC)</stronghuge> ,&nbsp; 2020<br/>
      <li> <stronghuge>National Inspirational Scholarship</stronghuge>,&nbsp; 2018<br/>
      <li> <stronghuge>Selected entrant for Deepcamp 2020</stronghuge> (200 people worldwide),&nbsp; 2020<br/>
      <li> <stronghuge>Outstanding Camper of Tencent Rhino Bird Elite Research Camp</stronghuge> (24 people worldwide),&nbsp; 2020<br/>  
      <li> <stronghuge>Selected entrant for Google Machine Learning Winter Camp 2019</stronghuge> (100 people worldwide),&nbsp; 2019<br/>    
      <li>  <stronghuge>China Collegiate Programming Contest (ACM-CCPC), JiLin, Bronze</stronghuge>,&nbsp; 2018 <br/>
		  <li> <stronghuge>Outstanding Student Scholarship</stronghuge> (Top 10% student), UESTC&nbsp; 2017~2019<br/>
	<li> <stronghuge> Peking University Excellent Graduation Thesis<stronghuge> (Top 10%), PKU&nbsp;  2023<br/>
          </p>
          </div>
        </td>
      </tr>
</table>

<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="40%">
          <img src='image/rushrushrush.jpg', width="400" height="200">
        </td>
        <td width="100%" valign="middle">
          <heading>Academic Service</heading>
          <div style="line-height:25px">
          <p>
		  <li> <stronghuge>PC Member:</stronghuge> &nbsp; CVPR'23/24, NeurIPS'22/23, ICLR'23/24, ICCV'23, ACM MM'24, ECCV'24<br/>
		  <li> <stronghuge>Journal Reviewer:</stronghuge> &nbsp; IEEE TCSVT, IEEE TPAMI<br/>  
          </p>
          </div>
        </td>
      </tr>
</table>

<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Personal Interests</heading>
          <p>
          <stronghuge><strong>Anime</strong>: As a pastime in my spare time, I watched a lot of Japanese anime about love, sports, and sci-fi. </stronghuge>
          </p>
          <p>
          <literature><strong>Literature</strong>: My favorite writer is <a href="https://en.wikipedia.org/wiki/Wang_Xiaobo">Xiaobo Wang</a>, the wisdom of his life inspires me. My favorite philosopher is <a href="https://en.wikipedia.org/wiki/Friedrich_Nietzsche">Friedrich Wilhelm Nietzsche</a>, and I am grateful that his philosophy has accompanied me through many difficult times in my life. </stronghuge>
          </p>     

      </td>
      </tr>

<p></p><p></p><p></p><p></p><p></p>
<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=g8JfCuglsVHk6L-tAQR2jVnplj3XNoA-rUSHfEZOf8M&cl=ffffff&w=a"></script>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td>
      <br>
      <p align="left"><font size="2">
        <a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Finfaaa.github.io&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"/></a>
      <p align="left"><font size="2">
      Last updated on Feb, 2024.
      <p align="middle"><font size="2"></font>      
      This awesome template borrowed from <a href="https://people.eecs.berkeley.edu/~barron/">this good manðŸ’—ðŸ’—ðŸ’—.</a>
      </tbody></table>

</body>
</html>
