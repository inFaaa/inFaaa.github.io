<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>JinFa Huang</title>
  
  <meta name="author" content="JinFa Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="image/pku_logo.png">
</head>
 
  <body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>JinFa Huang &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		  <img style="vertical-align:middle" src='image/vhjf_name.png' height='90px' width='WIDTHpx'>
		  </name>
        </p>
		<p>I am currently a final-year master student at <a href="https://web.pkusz.edu.cn/ldm/"></a>Digital Media Research Center</aJi> of <a href="https://www.pku.edu.cn/">Peking University (PKU)</a>, supervised by <a href="https://sites.google.com/view/jie-chen-2020/home">Prof. Jie Chen</a>. 
    Prior to that, I obtained the honoured bachelor degree from <a href="https://en.uestc.edu.cn/">University of Electronic Science and Technology of China (UESTC)</a> in July 2020.  
		</p>
    <p>I have been lucky to work with Dr. Yang Chao during my internship at <a href="https://www.tencent.com/en-us/about.html">Tencent IEG</a>, Dr. <a href="https://yeezhu.github.io/">Yi Zhu</a> and Prof. <a href="https://scholar.google.com/citations?user=sKauaAwAAAAJ&hl=zh-CN">Jianzhuang Liu</a> during my intership at <a href="http://dev3.noahlab.com.hk/">Huawei Noah's Ark Lab</a>,
	  and Dr. <a href="https://scholar.google.com/citations?user=PhbfktIAAAAJ&hl=zh-CN">Guoli Song</a> during my intership at <a href="https://www.pcl.ac.cn/">PengCheng Lab</a>,
    as well as Dr. <a href="https://sites.google.com/view/yanan-wang">Yanan Wang</a> and Dr. <a href="https://www.linkedin.cn/incareer/in/jianming-wu-5149748b">Jianming Wu</a> when I was a research intern at <a href="https://www.kddi-research.jp/english">KDDI Research</a>.
    </p>
      I aim at building multimodal interactive AI systems that can not only ground and reason over the external world signals, e.g., vision, audio and knowledge, to understand human language, but also assist humans in decision-making and efficiently solving social concerns, e.g., robot. 
      As steps towards this goal,  my research interests include but are not limited to <strong>Visual Captioning</strong>, <strong>Video-Text Retrieval</strong> and <strong>Visual Prompt Tuning</strong>.
    </p>
      <strong> (Note: I am actively looking for any collaboration. Please feel free to contact me.)</strong>


        <p align=center>
          <a href="mailto:vhjf305@gmail.com">Email</a> &nbsp/&nbsp
          <a href="image/CV_Jinfa_Huang_PKU_en.pdf">CV</a> &nbsp/&nbsp
	  <a href="https://scholar.google.com/citations?user=2t7iBnkAAAAJ&hl=en&oi=ao">Google Scholar</a>  &nbsp/&nbsp
          <a href="https://github.com/inFaaa">Github</a> 

        </p>

        </td>
        <td width="55%">
        <img src="image/huangjf1115.jpeg" width="300">
        </td>
      </tr>
      </table>


<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>News</heading>
          <p>
		   <li> <strongsmall>[2022/09]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by ICRA 2023.</smalll><br/>
		  <li> <strongsmall>[2022/09]</strongsmall> &nbsp;&nbsp;<smalll>1 paper (Spotlight) is accepted by NeurIPS 2022.</smalll><br/>
          </p>
        </td>
      </tr>
</table>




      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Education</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <img src='image/uestc_icon1.png' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>University of Electronic Science and Technology of China (UESTC), China</stronghuge><br />
          Bachelor Degree in Software Engineering &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2016 - Jun. 2020 <br />
          GPA: <strong>3.92</strong>/4.00, &nbsp;&nbsp;Ranking: <strong>5/171</strong> <br />
          Supervisors: Prof. <a href="https://sites.google.com/site/uestcluo/">Xucheng Luo</a>
          </p>
        </td>
      </tr>
	  
	    <tr>
          <td width="10%">
            <img src='image/pku_logo.png' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>Peking University (PKU), China</stronghuge><br />
          MPhil Student in Computer Science &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2020 - Present <br />
          GPA: <strong>3.72</strong>/4.00, &nbsp;&nbsp;Artificial Intelligence (A+), Computer Vision (A+)<br />
          Supervisor: Prof. <a href="https://sites.google.com/view/jie-chen-2020/home">Jie Chen</a>
          </p>
        </td>
      </tr>
	  
      </table>





<p></p><p></p><p></p><p></p><p></p>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research Experience</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <a href="https://guangzi.qq.com/">
            <img src='image/Tencent-Logo.png' width="100">
            </a>
          </td>

          <td width="80%" valign="middle">
          <p>
          <stronghuge>IEG Lightspeed & Quantum Studios Technology Center, Tencent</stronghuge><br />
          <huge><em>Engineering Intern</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Jan. 2019 - Jun. 2019 <br />
          Advisors: &nbsp; Dr. Yang Chao<br/>
          <!-- <li> Proposed several novel methods for cross-modal retrieval which achieves the state-of-the-art performance on image-text matching.<br/>
          <li> Combined the GCN with Visual Question Generation Task and further boost the performance on an unexplored challenging task zero-shot VQA. <br/>
          <li> Complete 3 works and make the submission. --> 
          </p>
        </td>
      </tr>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <a href="https://www.kddi-research.jp/english">
            <img src='image/KDDI_logo.jpeg' width="100">
          </a>
          </td>

          <td width="80%" valign="middle">
          <p>
          <stronghuge>Multimedia Computing Lab, KDDI Research</stronghuge><br />
          <huge><em>Research  Intern</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Nov. 2019 - Jan. 2020 <br />
          Advisors: &nbsp;Dr. Yanan Wang and Dr. Jianming Wu </a><!--
          <li> Proposed several novel methods for cross-modal retrieval which achieves the state-of-the-art performance on image-text matching.<br/>
          <li> Comnined the GCN with Visual Question Generation Task and further boost the performance on an unexplored challenging task zero-shot VQA. <br/>
          <li> Complete 3 works and make the submission.-->
          </p>
        </td>
      </tr>
  
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <a href="http://www.szpclab.com/">
            <img src='image/pcl_logo.jpeg' width="100">
          </a>
          </td>

          <td width="80%" valign="middle">
          <p>
          <stronghuge>Medical Imaging Analysis Group, Pengcheng Lab<Label></Label></stronghuge><br />
          <huge><em>Research  Intern</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Sep. 2020 - March. 2022 <br />
          Advisors: &nbsp;Dr. Guoli Song and Pro. Jie Chen </a><!--
          <li> Proposed several novel methods for cross-modal retrieval which achieves the state-of-the-art performance on image-text matching.<br/>
          <li> Comnined the GCN with Visual Question Generation Task and further boost the performance on an unexplored challenging task zero-shot VQA. <br/>
          <li> Complete 3 works and make the submission.-->
          </p>
        </td>
      </tr>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	 <tr>
          <td width="10%">
            <a href="http://dev3.noahlab.com.hk//">
            <img src='image/Noah_logo.png' width="100">
          </a>
          </td>
		 
          <td width="80%" valign="middle">
          <p>
          <stronghuge>Computer Vision Research, Huawei Noah's Ark Lab<Label></Label></stronghuge><br />
          <huge><em>Research  Intern</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; March. 2022 - Present <br />
          Advisors: &nbsp;Dr. Yi Zhu and Pro. Jianzhuang Liu </a>
          </p>
        </td>
      </tr>

<!-- <p></p><p></p>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publication & Manuscript
        </td>
      </tr>
      </table>

   
  
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/vc-rcnn/framework_github.png'  width="200" height="110">
      </td>
      <td valign="top" width="75%">
    <a href="#">
            <papertitle>Cross-stream Selective Networks for Action Recognition</papertitle>
    </a>
    <br>


	 <strong>Visual Commonsense R-CNN</strong><br>
       <strong>Tan Wang</strong>,
    <a>Jianqiang Huang</a>,
    <a href="https://www.ntu.edu.sg/home/hanwangzhang/">Hanwang Zhang</a>,
      <a href="https://qianrusun.com/">Qianru Sun</a> <br>
        <em>IEEE International Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2020</strong>, <a href="https://arxiv.org/abs/2002.12204"><strong>[Paperlink]</strong></a>, 
		<a href="https://github.com/Wangt-CN/VC-R-CNN"><strong>[Code]</strong></a>, <a href="https://zhuanlan.zhihu.com/p/111306353"><strong>[Zhihu]</strong></a></em><br>
        <em>Area: Visual and Language, Causal Reasoning, Self-supervised Learning</em> <br>
        <p></p>
		<p>In this paper, we present a novel un-/self-supervised feature representation learning method, Visual Commonsense Region-based Convolutional Neural Network (VC R-CNN), to serve as an improved visual region encoder for Vision & Language high-level tasks. </p>
      </td>
    </tr>
   </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/vc-rcnn/VC_CVPRW.png'  width="200" height="110">
      </td>
      <td valign="top" width="75%">
    <a href="#">
            <papertitle>Cross-stream Selective Networks for Action Recognition</papertitle>
    </a>
    <br>

	 <strong>Visual Commonsense Representation Learning via Causal Inference (Abstact Version of VC R-CNN)</strong><br>
       <strong>Tan Wang</strong>,
    <a>Jianqiang Huang</a>,
    <a href="https://www.ntu.edu.sg/home/hanwangzhang/">Hanwang Zhang</a>,
      <a href="https://qianrusun.com/">Qianru Sun</a> <br>
        <em>IEEE International Conference on Computer Vision and Pattern Recognition MVM Workshop, <strong>CVPRW 2020</strong>, <a href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w26/Wang_Visual_Commonsense_Representation_Learning_via_Causal_Inference_CVPRW_2020_paper.html"><strong>[Paperlink]</strong></a>, <a href="https://github.com/Wangt-CN/VC-R-CNN"><strong>[Code]</strong></a>, <a href="https://zhuanlan.zhihu.com/p/111306353"><strong>[Zhihu]</strong></a></em><br>
        <em><strong><font color="#a82e2e">(Oral Presentation)</font></strong></em> <br>
		<em>Area: Visual and Language, Causal Reasoning, Self-supervised Learning</em> <br>
      </td>
    </tr>
   </table> -->


<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Selected Honors & Scholarships</heading>
          <div style="line-height:25px">
          <p>
		  <li> <stronghuge>Outstanding Graduate of University of Electronic Science and Technology of China (UESTC)</stronghuge> ,&nbsp; 2020<br/>
      <li> <stronghuge>National Inspirational Scholarship</stronghuge>,&nbsp; 2018<br/>
      <li> <stronghuge>Selected entrant for Deepcamp 2020</stronghuge> (200 people worldwide),&nbsp; 2020<br/>
      <li> <stronghuge>Outstanding Camper of Tencent Rhino Bird Elite Research Camp</stronghuge> (24 people worldwide),&nbsp; 2020<br/>  
      <li> <stronghuge>Selected entrant for Google Machine Learning Winter Camp 2019</stronghuge> (100 people worldwide),&nbsp; 2019<br/>    
      <li>  <stronghuge>Chinese university student programming contest (ACM-CCPC), JiLin, Bronze</stronghuge>,&nbsp; 2018 <br/>
		  <li> <stronghuge>Outstanding Student Scholarship</stronghuge> (Top 10% student), UESTC&nbsp; 2017~2019<br/>
          </p>
          </div>
        </td>
      </tr>
</table>


<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Personal Interests</heading>
          <p>
          <stronghuge><strong>Anime</strong>: As a pastime in my spare time, I watched a lot of Japanese anime about love, sports and sci-fi. </stronghuge>
          </p>
          <p>
          <literature><strong>Literature</strong>: My favorite writer is Xiaobo Wang, the wisdom of his life inspires me. My favorite philosopher is Friedrich Wilhelm Nietzsche, and I am grateful that his philosophy has accompanied me through many difficult times in my life. </stronghuge>
          </p>     
      </td>
      </tr>


<p></p><p></p><p></p><p></p><p></p>
      <!-- <a href="https://clustrmaps.com/site/1bnt7"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=g8JfCuglsVHk6L-tAQR2jVnplj3XNoA-rUSHfEZOf8M&cl=ffffff" /></a> -->
      <!-- <script type="text/javascript" id="clustrmaps" src="//www.clustrmaps.com/map_v2.png?d=g8JfCuglsVHk6L-tAQR2jVnplj3XNoA-rUSHfEZOf8M&cl=ffffff"></script>       -->
      <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=g8JfCuglsVHk6L-tAQR2jVnplj3XNoA-rUSHfEZOf8M&cl=ffffff&w=a"></script>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

			<tbody><tr>
				<td>
				<br>
				<p align="left"><font size="2">
        Last updated on Sep, 2022
				<p align="middle"><font size="2">
				This awesome template are obtained from <a href="https://people.eecs.berkeley.edu/~barron/">this good man</a>!
				</tbody></table>

</body>
</html>
