<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>𝑱𝙞𝒏𝙛𝒂 𝑯𝙪𝒂𝙣𝒈</title>
  
  <meta name="author" content="Jinfa Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="image/logo_hjf.png">
  
</head>
 
  <body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>𝒥𝒾𝓃𝒻𝒶 ℋ𝓊𝒶𝓃ℊ&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		        <img style="vertical-align:middle" src='image/vhjf_name.png' height='90px' width='WIDTHpx'>
		        </name>
        </p>
		<p>
      <p>✨Bonjour, I am a Ph.D. student in the Department of Computer Science, <a href="https://www.rochester.edu/">University of Rochester (UR)</a>, advised by <a href="https://www.cs.rochester.edu/u/jluo/">Prof. Jiebo Luo</a>. </p>
      <p>I aim at building multimodal interactive AI systems that can not only ground, reason and generate over the external world signals, to understand human language, but also assist humans in decision-making and efficiently solving social concerns, e.g., robot, medical. 
        As steps towards this goal, my research interests include but are not limited to <strong>multimodal understanding</strong>, <strong>multimodal generation</strong> and  <strong>multimodal foundation model post-training</strong>.
      </p>
        Prior to that, I got my master's degree from <a href="https://www.pku.edu.cn/">Peking University (PKU)</a> in 2023, advised by <a href="https://yuanli2333.github.io/">Prof. Li Yuan</a> and <a href="https://sites.google.com/view/jie-chen-2020/home">Prof. Jie Chen</a>. 
    And I obtained the honored bachelor's degree from <a href="https://en.uestc.edu.cn/">University of Electronic Science and Technology of China (UESTC)</a> in 2020.  
    </p>


        <p align=center>
          <a href="mailto:vhjf305@gmail.com">Email</a> &nbsp/&nbsp
	        <a href="https://scholar.google.com/citations?user=2t7iBnkAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
          <a href="https://github.com/inFaaa">Github</a> &nbsp/&nbsp
          <a href="https://twitter.com/vhjf36495872?s=21&t=F9wouyOg6_aJ83H0oZ92yQ">Twitter</a> &nbsp/&nbsp
          <a href="https://www.zhihu.com/people/vm2016">Zhihu</a>  &nbsp/&nbsp
          <a href="https://www.linkedin.com/in/jinfa-huang-262a2929b/">LinkedIn</a> 
        </p>

        </td>
        <td width="33%">
          <img src="image/photo.jpg" width="300" onmouseover="this.src='image/photo5.jpg';" />
          <div style="text-align: center;"><em>Winter 2024, Puerto Rico✨</em></div>
      </td>
      <!-- <td style="padding: 10% 2% 10% 2%;width:70%;max-width:70%">
        <img src="images/photo.jpg" style="float:center" width="78%" alt="photo">
        <figcaption style="font-size: smaller; color: #A9A9A9;">@Aurora, Rochester, Oct. 2024  </figcaption>
      </td> -->
      </tr>
  </table> 

<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
        <td width="100%" valign="middle">
            <heading>News</heading>
            <p>
                <li> <strongsmall>[2025/04]</strongsmall> &nbsp;&nbsp;<smalll> 1 paper (MagicTime) is accepted by TPAMI 2025.</smalll><br/>  
                <li> <strongsmall>[2025/03]</strongsmall> &nbsp;&nbsp;<smalll> 1 paper (AR-Visual Survey) is accepted by TMLR 2025.</smalll><br/>  
                <li> <strongsmall>[2025/02]</strongsmall> &nbsp;&nbsp;<smalll> 1 paper (ConsisID) is accepted by CVPR 2025.</smalll><br/>  
                <li> <strongsmall>[2025/01]</strongsmall> &nbsp;&nbsp;<smalll> 2 papers (1 Poster and 1 Spotlight) are accepted by ICLR 2025.</smalll><br/>  
                <li> <strongsmall>[2025/01]</strongsmall> &nbsp;&nbsp;<smalll> 1 paper (Medical LLM Survey) is accepted by Nature Reviews Bioengineering 2025.</smalll><br/>  
                <li class="hidden-news"> <strongsmall>[2025/01]</strongsmall> &nbsp;&nbsp;<smalll> Started the research internship at Google, USA, supervised by <a href="https://scholar.google.com/citations?user=umEBJkQAAAAJ&hl=en">Jiageng Zhang</a> and <a href="https://scholar.google.com/citations?user=Nun8Dy0AAAAJ&hl=en&authuser=1">Dr. Eric Li</a>.</smalll><br/>
                <li class="hidden-news"><strongsmall>[2024/12]</strongsmall> &nbsp;&nbsp;<smalll> Happy New Year🥳! 1 paper (PromptLLM) is accepted by TPAMI 2025.</smalll><br/>  
                <li class="hidden-news"><strongsmall>[2024/12]</strongsmall> &nbsp;&nbsp;<smalll> 1 paper is accepted by AAAI 2025.</smalll><br/>  
                <li class="hidden-news"><strongsmall>[2024/12]</strongsmall> &nbsp;&nbsp;<smalll> 1 short paper is accepted by COLING 2025.</smalll><br/>  
                <li class="hidden-news"><strongsmall>[2024/11]</strongsmall> &nbsp;&nbsp;<smalll> 1 paper is accepted by ACM Transactions on Intelligence Systems and Technology (TIST) 2024.</smalll><br/>  
                <li class="hidden-news"><strongsmall>[2024/11]</strongsmall> &nbsp;&nbsp;<smalll> Winter is coming❄️! 1 paper is accepted by npj Digital Medicine (Impact Factor: 15.357).</smalll><br/>  
                <li class="hidden-news"><strongsmall>[2024/11]</strongsmall> &nbsp;&nbsp;<smalll> 1 survey is accepted by CAAI Transactions on Intelligence Technology (Impact Factor: 8.4), which aims at promoting camouflaged object detection and beyond tasks: <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ChunmingHe/awesome-concealed-object-segmentation?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/ChunmingHe/awesome-concealed-object-segmentation">Awesome Concealed Object Segmentation</a></smalll>.<br/> 
                <li class="hidden-news"><strongsmall>[2024/10]</strongsmall> &nbsp;&nbsp;<smalll> 🔥🔥🔥 We release a GitHub repository and survey aim at promoting the application of autoregressive models in vision domain: <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ChaofanTao/Autoregressive-Models-in-Vision-Survey?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey">Awesome Autoregressive Models in Vision</a></smalll>.<br/> 
                <li class="hidden-news"><strongsmall>[2024/09]</strongsmall> &nbsp;&nbsp;<smalll> 1 paper (Spotlight) is accepted by NeurIPS 2024 Datasets & Benchmarks Track.</smalll><br/>  
                <li class="hidden-news"><strongsmall>[2024/09]</strongsmall> &nbsp;&nbsp;<smalll> 1 paper is accepted by EMNLP 2024 Findings.</smalll><br/>              
                <li class="hidden-news"><strongsmall>[2024/06]</strongsmall> &nbsp;&nbsp;<smalll>🔥🔥🔥 We are excited to present <a href="https://pku-yuangroup.github.io/ChronoMagic-Bench/">𝐂𝐡𝐫𝐨𝐧𝐨𝐌𝐚𝐠𝐢𝐜-𝐁𝐞𝐧𝐜𝐡</a>, a benchmark for metamorphic evaluation of text-to-video generation, which provides valuable insights for T2V models selection.  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PKU-YuanGroup/ChronoMagic-Bench?logoColor=%23C8A2C8&color=%23DCC6E0"> </smalll><br/>
                <li class="hidden-news"><strongsmall>[2024/05]</strongsmall> &nbsp;&nbsp;<smalll> Started the research internship at ByteDance Seed, Bellevue, USA, supervised by  <a href="https://qzyou.github.io/">Quanzeng You</a> & <a href="https://sites.google.com/view/yongfei-liu/">Yongfei Liu</a> & <a href="https://scholar.google.com/citations?user=B1EhbCsAAAAJ&hl=en">Jianbo Yuan</a>.</smalll><br/>
                <li class="hidden-news"><strongsmall>[2024/05]</strongsmall> &nbsp;&nbsp;<smalll> 1 paper is accepted by ACL 2024 Findings.</smalll><br/></li>
                <li class="hidden-news"><strongsmall>[2024/04]</strongsmall> &nbsp;&nbsp;<smalll>🔥🔥🔥 We are thrilled to present <a href="https://pku-yuangroup.github.io/MagicTime">𝐌𝐚𝐠𝐢𝐜𝐓𝐢𝐦𝐞</a>, a metamorphic time-lapse video generation model and a new dataset ChronoMagic, support U-Net or DiT-based T2V frameworks.  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PKU-YuanGroup/MagicTime?logoColor=%23C8A2C8&color=%23DCC6E0"> </a></smalll><br/>
                <li class="hidden-news"><strongsmall>[2024/01]</strongsmall> &nbsp;&nbsp;<smalll> 1 paper is accepted by ICLR 2024.</smalll><br/>
                <li class="hidden-news"><strongsmall>[2023/11]</strongsmall> &nbsp;&nbsp;<smalll>🔥🔥🔥 We release a GitHub repository to promote medical Large Language Models research with the vision of applying LLM to real-life medical scenarios: <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/AI-in-Health/MedLLMsPracticalGuide?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/AI-in-Health/MedLLMsPracticalGuide">A Practical Guide for Medical Large Language Models. </a></smalll><br/> 
                <li class="hidden-news"><strongsmall>[2023/11]</strongsmall> &nbsp;&nbsp;<smalll>🔥🔥🔥 How could LMMs contribute to social good? We are excited to release a new preliminary explorations of GPT-4V(ison) for social multimedia: <a href="https://arxiv.org/abs/2311.07547">GPT-4V(ision) as A Social Media Analysis Engine.</a> </smalll><br/>          
                <li class="hidden-news"><strongsmall>[2023/09]</strongsmall> &nbsp;&nbsp;<smalll>Join the  <a href="https://www.cs.rochester.edu/u/jluo/#Prospective">VIStA Lab</a> as a Ph.D. student working on vision and language.</smalll><br/>
                <li class="hidden-news"><strongsmall>[2023/07]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by ACMMM 2023.</smalll><br/>
                <li class="hidden-news"><strongsmall>[2023/05]</strongsmall> &nbsp;&nbsp;<smalll>I was awarded the 2023 Peking University Excellent Graduation Thesis.</smalll><br/>
                <li class="hidden-news"><strongsmall>[2023/04]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by TIP 2023.</smalll><br/>
                <li class="hidden-news"><strongsmall>[2023/04]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by IJCAI 2023.</smalll><br/>
                <li class="hidden-news"><strongsmall>[2023/02]</strongsmall> &nbsp;&nbsp;<smalll>1 paper (Top 10% Highlight) is accepted by CVPR 2023.</smalll><br/>
                <li class="hidden-news"> <strongsmall>[2022/09]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by ICRA 2023.</smalll><br/>
                <li class="hidden-news"> <strongsmall>[2022/09]</strongsmall> &nbsp;&nbsp;<smalll>1 paper (Spotlight) is accepted by NeurIPS 2022.</smalll><br/> 
            </p>
        </td>
    </tr>
</table>

<button onclick="toggleNews()">Show More</button>

<script>
function toggleNews() {
    var hiddenNews = document.querySelectorAll('.hidden-news');

    hiddenNews.forEach(function(news) {
        if (news.style.display === 'none' || news.style.display === '') {
            news.style.display = 'list-item';
        } else {
            news.style.display = 'none';
        }
    });
}
</script>


<p></p><p></p><p></p><p></p><p></p>
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Education</heading>
        </td>
      </tr>
      </table>
  
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
      <td width="10%">
        <img src='image/UR_logo.webp' width="100">
      </td>

      <td width="75%" valign="middle">
      <p>
      <stronghuge>University of Rochester (UR), USA</stronghuge><br />
      PH.D. Student in Computer Science &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2023 - Present <br />
      Advisor: <a href="https://www.cs.rochester.edu/u/jluo/">Prof. Jiebo Luo</a>
      </p>
    </td>
  </tr>

  <tr>
      <td width="10%">
        <img src='image/pku_logo.png' width="100">
      </td>

      <td width="75%" valign="middle">
      <p>
      <stronghuge>Peking University (PKU), China</stronghuge><br />
      Master Degree in Computer Science &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2020 - Jun. 2023 <br />
      Advisors:  <a href="https://yuanli2333.github.io/">Prof. Li Yuan</a> and <a href="https://sites.google.com/view/jie-chen-2020/home">Prof. Jie Chen</a>
      </p>
    </td>
  </tr>

    <tr>
        <td width="10%">
          <img src='image/uestc_icon1.png' width="100">
        </td>

        <td width="75%" valign="middle">
        <p>
        <stronghuge>University of Electronic Science and Technology of China (UESTC), China</stronghuge><br />
        Bachelor Degree in Software Engineering &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2016 - Jun. 2020 <br />
        Advisors:  <a href="https://en.uestc.edu.cn/info/1074/2171.htm">Prof. Xucheng Luo</a> 
        </p>
      </td>
    </tr>
  </table>

  <p></p><p></p><p></p><p></p><p></p>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
    <td width="100%" valign="middle">
      <heading>Research Experience</heading>
    </td>
  </tr>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="12%">
        <a href="https://research.google/">
        <img src='image/google_icon2.png' width="100">
      </a>
      </td>

      <td width="80%" valign="middle">
        <p>
        <stronghuge>Core ML Applied ML, Google, USA</stronghuge><br />
        <huge><em>Student Research</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Jan. 2025 - Now <br />
        Advisors: &nbsp;  <a href="https://scholar.google.com/citations?user=umEBJkQAAAAJ&hl=en">Jiageng Zhang</a> and <a href="https://scholar.google.com/citations?user=Nun8Dy0AAAAJ&hl=en&authuser=1">Dr. Eric Li</a>.
        </p>
      </td>
    </tr>
  </tr>



  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="10%">
        <a href="https://www.bytedance.com/en/">
        <img src='image/bytedance_logo.png' width="100">
        </a>
      </td>

      <td width="80%" valign="middle">
      <p>
      <stronghuge>Seed-Foundation-Model, ByteDance</stronghuge><br />
      <huge><em>Research Intern</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; May. 2024 - Aug. 2024 <br />
      Advisors: &nbsp;  <a href="https://qzyou.github.io/">Dr. Quanzeng You</a> & <a href="https://sites.google.com/view/yongfei-liu/">Dr. Yongfei Liu</a> & <a href="https://scholar.google.com/citations?user=B1EhbCsAAAAJ&hl=en">Dr. Jianbo Yuan</a>
      </p>
    </td>
  </tr>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="12%">
        <a href="https://www.pcl.ac.cn/">
        <img src='image/pcl_logo.jpeg' width="100">
      </a>
      </td>

      <td width="80%" valign="middle">
      <p>
      <stronghuge>Artificial Intelligence Center, Pengcheng Lab</stronghuge><br />
      <huge><em>Research Intern</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Sep. 2020 - Aug. 2022 <br />
      Advisors: &nbsp;  <a href="https://songguoli87.github.io/pcl/">Dr. Guoli Song</a> & <a href="https://sites.google.com/view/jie-chen-2020/home">Prof. Jie Chen</a>
      </p>
    </td>
  </tr>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="15%">
        <a href="https://www.kddi-research.jp/english">
        <img src='image/KDDI_logo.jpeg' width="100">
      </a>
      </td>

      <td width="80%" valign="middle">
      <p>
      <stronghuge>Multimedia Computing Team, KDDI Research</stronghuge><br />
      <huge><em>Research Intern</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Nov. 2019 - Feb. 2020 <br />
      Advisors: &nbsp;  <a href="https://sites.google.com/view/yanan-wang">Dr. Yanan Wang</a> & <a href="https://scholar.google.com/citations?user=GMENvgUAAAAJ&hl=ja">Dr. Jianming Wu</a>
      </p>
    </td>
  </tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="10%">
        <a href="https://www.tencent.com/">
        <img src='image/Tencent-Logo.png' width="100">
      </a>
      </td>

      <td width="80%" valign="middle">
      <p>
      <stronghuge>X-Data Research Group, Tencent IEG</stronghuge><br />
      <huge><em>Engineering Intern</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Jan. 2019 - Jul. 2019 <br />
      Advisors: &nbsp; Boya Yin & Dr. Yang Chao
      </p>
    </td>
  </tr>

<p></p><p></p><p></p><p></p><p></p>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Selected Publication</heading>
          <p>
            My current research mainly focuses on the vision+language, generative model. *Equal Contribution. 
            </p>
        </td>
      </tr>
      </table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" bgcolor="#ffffd0">
    <td width="15%">
      <img src='project/MagicTime_Arxiv2024.png', width="200" height="150">
    </td>
    <td valign="top" width="100%">
  <strong>MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators</strong><br>
  <a>Shenghai Yuan*</a>,
  <strong>Jinfa Huang*</strong>,
  <a>Yujun Shi</a>,
  <a>Yongqi Xu</a>,
  <a>Ruijie Zhu</a>,
  <a>Bin Lin</a>,     
  <a>Xinhua Cheng</a>,   
  <a>Li Yuan</a>,   
  <a>Jiebo Luo</a>   
  <br>
  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence, <strong>TPAMI 2025</strong></em><br>
      <em><strong><font color="#a82e2e">(Github Repo 1300+ Stars🌟)</font></strong></em> <br>
      <a href="https://arxiv.org/abs/2404.05014">[Paperlink]</a>, <a href="https://github.com/PKU-YuanGroup/MagicTime">[Code]</a>, <a href="https://pku-yuangroup.github.io/MagicTime/">[Page]</a>, <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PKU-YuanGroup/MagicTime?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/PKU-YuanGroup/MagicTime"></a> <br>
      <em><strong>Area:</strong> Text-to-Video Generation, Diffusion Model, Time-lapse Videos</em> <br>
      <p></p>
  <p>Existing text-to-video generation models have not adequately encoded physical knowledge of the real world, thus generated videos tend to have limited motion and poor variations.
     In this paper, we propose MagicTime, a metamorphic time-lapse video generation model, which learns real-world physics knowledge from time-lapse videos and implements metamorphic video generation. </p>
    </td>
  </tr>
        </table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" bgcolor="#ffffd0">
    <td width="15%">
      <img src='project/HBI_CVPR2023.png', width="200" height="150">
    </td>
    <td valign="top" width="100%">
  <strong>Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning</strong><br>
  <a href="https://jpthu17.github.io/">Peng Jin</a>,
  <strong>Jinfa Huang</strong>,
  <a>Pengfei Xiong</a>,
  <a>Shangxuan Tian</a>,    
  <a>Chang Liu</a>,
  <a>Xiangyang Ji</a>,
  <a>Li Yuan</a>,
  <a>Jie Chen</a>
  <br>  
      <em>IEEE International Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2023</strong></em><br>
      <em><strong><font color="#a82e2e">(Highlight, Top 2.5%)</font></strong></em> <br>
  <a href="https://arxiv.org/abs/2303.14369">[Paperlink]</a>, <a href="https://github.com/jpthu17/HBI">[Code]</a>, <a href="https://pku-yuangroup.github.io/MagicTime/">[Page]</a>, <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/jpthu17/HBI?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/jpthu17/EMCL"></a> <br>
  <em><strong>Area:</strong> Video-and-Language Representation, Machine Learning, Video-Text Retrieval, Video Captioning</em> <br>
      <p></p>
  <p>To solve the problem of the modality gap in video-text feature space, we propose Expectation-Maximization Contrastive Learning (EMCL) to learn compact video-and-language representations. 
    We use the Expectation-Maximization algorithm to find a compact set of bases for the latent space, where the features could be concisely represented as the linear combinations of these bases.</p>
    </td>
    </td>
  </tr>
       </table>
  
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
<tr onmouseout="aperture_stop()" onmouseover="aperture_start()" bgcolor="#ffffd0">
  <td width="15%">
    <img src='project/EMCL_NIPS2022.png', width="200" height="150">
  </td>
  <td valign="top" width="100%">
<strong>Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations</strong><br>
<a href="https://jpthu17.github.io/">Peng Jin*</a>,
<strong>Jinfa Huang*</strong>,
<a>Fenglin Liu</a>,
<a>Xian Wu</a>,    
<a>Shen Ge</a>,
<a>Guoli Song</a>,
<a>David A. Clifton</a>,
<a>Jie Chen</a>
<br>
    <em>Conference on Neural Information Processing Systems, <strong>NeurIPS 2022</strong></em><br>
<em><strong><font color="#a82e2e">(Spotlight Presentation, Top 5%)</font></strong></em> <br>

<a href="https://arxiv.org/abs/2211.11427">[Paperlink]</a>, <a href="https://github.com/jpthu17/EMCL">[Code]</a>, <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/jpthu17/EMCL?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/jpthu17/EMCL"></a> <br>
    <em><strong>Area:</strong> Video-and-Language Representation, Machine Learning, Video-Text Retrieval, Video Captioning</em> <br>
    <p></p>
<p>To solve the problem of the modality gap in video-text feature space, we propose Expectation-Maximization Contrastive Learning (EMCL) to learn compact video-and-language representations. We use the Expectation-Maximization algorithm to find a compact set of bases for the latent space, where the features could be concisely represented as the linear combinations of these bases.</p>
  </td>
</tr>
      </table>

<!-- 
<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
<tr>
  <td width="100%" valign="middle">
    <heading>All Publication <a href="https://scholar.google.com/citations?hl=en&user=2t7iBnkAAAAJ" style="font-size:22px;">[Google Scholar]</a></heading>
    <p>
      My current research mainly focuses on multimodal generation and understanding. (*Equal Contribution)
      </p>
    <h3>arXiv preprints</h3>
    <p>
      [1] Bin Zhu, Peng Jin, Munan Ning, Bin Lin, <strong>Jinfa Huang</strong>, Qi Song, Mingjun Pan, Li Yuan. "LLMBind: A unified modality-task integration framework"
      [<a href="https://arxiv.org/abs/2402.14891">PDF</a>][<a href="https://github.com/PKU-YuanGroup/LLMBind">Code</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PKU-YuanGroup/LLMBind?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/PKU-YuanGroup/LLMBind"></a> 
    </p>
    <p>
      [2] Bin Lin, Zhenyu Tang, Yang Ye, <strong>Jinfa Huang</strong>, Junwu Zhang, Patian Pang, Peng Jin, Munan Ning, Jiebo Luo, Li Yuan. "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models"
      [<a href="https://arxiv.org/abs/2401.15947">PDF</a>][<a href="https://github.com/PKU-YuanGroup/MoE-LLaVA">Code</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PKU-YuanGroup/MoE-LLaVA?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/PKU-YuanGroup/MoE-LLaVA"></a> 
    </p>
    <p>
      [3] Cong Jin, Jingru Fan, <strong>Jinfa Huang</strong>, Jinyuan Fu, Yi Zhang, Tao Mei, Li Yuan, Jiebo Luo. "Next-Gen AIGC: Harnessing Advanced Multimodal Foundation Models for Text-to-Media Innovations"
    </p>
    <p>
      [4] Yongdong Luo, Xiawu Zheng, Xiao Yang, Guilin Li, Haojia Lin, <strong>Jinfa Huang</strong>, Jiayi Ji, Fei Chao, Jiebo Luo, Rongrong Ji. "Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension" 
      [<a href="https://arxiv.org/abs/2411.13093">PDF</a>][<a href="https://github.com/Leon1207/Video-RAG-master">Code</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Leon1207/Video-RAG-master?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/Leon1207/Video-RAG-master"></a> 
    </p>  
    <p>
      [5] Yongdong Luo, Chen Wang, Xiawu Zheng, Weizhong Huang, Shukang Yin, Haojia Lin, Chaoyou Fu, <strong>Jinfa Huang</strong>, Jiayi Ji, Jiebo Luo, Rongrong Ji. "QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long Video Comprehension" 
      [<a href="https://arxiv.org/abs/2503.08689">PDF</a>][<a href="https://github.com/MAC-AutoML/QuoTA">Code</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/MAC-AutoML/QuoTA?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/MAC-AutoML/QuoTA"></a> 
    </p>  
  

    <h3>2025</h3>
    <p>
      [1] <strong>Jinfa Huang*</strong>, Jinsheng Pan*, Zhongwei Wan, Hanjia Lyu, Jiebo Luo. "Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models for Hateful Meme Detection", <strong>COLING 2025</strong>, short paper,
      [<a href="https://aclanthology.org/2025.coling-main.489.pdf">PDF</a>] [<a href="https://github.com/inFaaa/Evolver">Code</a>] [<a href="https://github.com/inFaaa/Evolver/blob/main/assets/evolver_poster_COLING2025.png">Poster</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/inFaaa/Evolver?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/inFaaa/Evolver"></a>
    </p>
    <p>
      [2] Haoran Tang, Meng Cao, <strong>Jinfa Huang</strong>, Ruyang Liu, Peng Jin, Ge Li, Xiaodan Liang. "MUSE: Mamba is Efficient Multi-scale Learner for Text-video Retrieval", <strong>AAAI 2025</strong>,
      [<a href="https://arxiv.org/abs/2408.10575">PDF</a>][<a href="https://github.com/hrtang22/MUSE">Code</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/hrtang22/MUSE?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/hrtang22/MUSE"></a>
    </p>
    <p>
      [3] Fenglin Liu, Xian Wu, <strong>Jinfa Huang</strong>, Kim Branson, Patrick Schwab, Lei Clifton, Ping Zhang, Jiebo Luo, Yefeng Zheng, and David A. Clifton. "Aligning, Autoencoding and Prompting Large Language Models for Novel Thorax Disease Reporting", <strong>TPAMI 2025</strong>, 
      [<a href="https://arxiv.org/abs/2408.00000">PDF</a>][<a href="https://github.com/ai-in-health/PromptLLM">Code</a>] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ai-in-health/PromptLLM?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/ai-in-health/PromptLLM"></a>
    </p>
    <p>
      [4] Hongjian Zhou*, Fenglin Liu*, Boyang Gu*, Xinyu Zou*, <strong>Jinfa Huang*</strong>, Jinge Wu, Yiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng Mao, Xian Wu, Yefeng Zheng, Lei Clifton, Zheng Li, Jiebo Luo, David A. Clifton. "A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges", <strong>Nature Reviews Bioengineering 2025</strong>, 
      [<a href="https://arxiv.org/abs/2311.05112">PDF</a>][<a href="https://github.com/AI-in-Health/MedLLMsPracticalGuide">Code</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/AI-in-Health/MedLLMsPracticalGuide?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/AI-in-Health/MedLLMsPracticalGuide"></a> 
    </p>
    <p>
      [5] Shaofeng Zhang, Qiang Zhou, Sitong Wu, Haoru Tan, Zhibin Wang, <strong>Jinfa Huang</strong>, Junchi Yan. "CR2PQ: Continuous Relative Rotary Positional Query for Dense Visual Representation Learning", <strong>ICLR 2025</strong>, 
      [<a href="https://openreview.net/forum?id=3l6PwssLNY&noteId=7WmiRtJQpw">PDF</a>][<a href="https://github.com/Sherrylone/PQRoPE">Code</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Sherrylone/PQRoPE?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/Sherrylone/PQRoPE"></a>
    </p>
    <p>
      [6] Chunming He, Chengyu Fang, Yulun Zhang, Longxiang Tang, <strong>Jinfa Huang</strong>, Kai Li, Zhenhua Guo, Xiu Li, Sina Farsiu. "Reti-Diff: Illumination Degradation Image Restoration with Retinex-based Latent Diffusion Model", <strong>ICLR 2025 Spotlight</strong>, 
      [<a href="https://openreview.net/pdf?id=kxFtMHItrf">PDF</a>][<a href="https://github.com/ChunmingHe/Reti-Diff">Code</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ChunmingHe/Reti-Diff?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/ChunmingHe/Reti-Diff"></a>
    </p>
    <p>
      [7] Fenglin Liu, Zheng Li, Qingyu Yin, <strong>Jinfa Huang</strong>, Xian Wu, Anshul Thakur, Kim Branson, Patrick Schwab, Bing Yin, Yefeng Zheng, Jiebo Luo, and David A. Clifton. "A Multimodal Multidomain Multilingual Medical Foundation Model for Zero-Shot Clinical Diagnosis", <strong>npj Digital Medicine</strong>,
      [<a href="https://arxiv.org/abs/2408.00000">PDF</a>][<a href="https://github.com/ai-in-health/M3FM">Github</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ai-in-health/M3FM?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/ai-in-health/M3FM"></a> 
    </p>
    <p>
      [8] Shenghai Yuan, <strong>Jinfa Huang</strong>, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, Li Yuan. "Identity-Preserving Text-to-Video Generation by Frequency Decomposition", <strong>CVPR 2025</strong>, 
      [<a href="https://arxiv.org/abs/2411.17440v3">PDF</a>][<a href="https://github.com/PKU-YuanGroup/ConsisID">Code</a>][<a href="https://pku-yuangroup.github.io/ConsisID">Page</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PKU-YuanGroup/ConsisID?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/PKU-YuanGroup/ConsisID"></a> 
    </p>    
    <p>
      [9] Jing Xiong, Gongye Liu, Lun Huang, Chengyue Wu, Taiqiang Wu, Yao Mu, Yuan Yao, Hui Shen, Zhongwei Wan, <strong>Jinfa Huang</strong>, Chaofan Tao, Shen Yan, Huaxiu Yao, Lingpeng Kong, Hongxia Yang, Mi Zhang, Guillermo Sapiro, Jiebo Luo, Ping Luo, Ngai Wong. "Autoregressive Models in Vision: A Survey", <strong>TMLR 2025</strong>,
      [<a href="https://arxiv.org/abs/2411.05902">PDF</a>][<a href="https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey">Code</a>] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ChaofanTao/Autoregressive-Models-in-Vision-Survey?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey"></a> 
    </p>  
    <p>
      [10] Shenghai Yuan*, <strong>Jinfa Huang*</strong>, Yujun Shi, Yongqi Xu, Ruijie Zhu, Bin Lin, Xinhua Cheng, Li Yuan, Jiebo Luo. "MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators", <strong>TPAMI 2025</strong>,
      [<a href="https://arxiv.org/abs/2404.05014">PDF</a>][<a href="https://github.com/PKU-YuanGroup/MagicTime">Code</a>][<a href="https://pku-yuangroup.github.io/MagicTime/">Project page</a>] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PKU-YuanGroup/MagicTime?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/PKU-YuanGroup/MagicTime"></a> 
    </p>
    <h3>2024</h3>
    <p>
      [1] Meng Cao*, Haoran Tang*, <strong>Jinfa Huang</strong>, Peng Jin, Can Zhang, Ruyang Liu, Long Chen, Xiaodan Liang, Li Yuan, Ge Li. "RAP: Efficient Text-Video Retrieval with Sparse-and-Correlated Adapter", <strong>ACL 2024 Finding</strong>,
      [<a href="https://arxiv.org/abs/2405.19465">PDF</a>]
    <p>
      [2] Shaofeng Zhang, <strong>Jinfa Huang</strong>, Qiang Zhou, Zhibin Wang, Fan Wang, Jiebo Luo, Junchi Yan. "Continuous-Multiple Image Outpainting in One-Step via Positional Query and A Diffusion-based Approach", <strong>ICLR 2024</strong>,
      [<a href="https://arxiv.org/abs/2401.15652">PDF</a>][<a href="https://github.com/Sherrylone/PQDiff">Code</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Sherrylone/PQDiff?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/Sherrylone/PQDiff"></a> 
    </p>
    <p>
      [3] Zhongwei Wan*, Ziang Wu*, Che Liu, <strong>Jinfa Huang</strong>, Zhihong Zhu, Peng Jin, Longyue Wang, Li Yuan. "LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference", <strong>EMNLP 2024 Finding</strong>,
      [<a href="https://arxiv.org/abs/2406.18139">PDF</a>][<a href="https://github.com/SUSTechBruce/LOOK-M">Code</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/SUSTechBruce/LOOK-M?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/SUSTechBruce/LOOK-M"></a> 
    </p>
    <p>
      [4] Shenghai Yuan, <strong>Jinfa Huang</strong>, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Ruijie Zhu, Xinhua Cheng, Jiebo Luo, Li Yuan. "ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation", <strong>NeurIPS 2024 D&B Spotlight</strong>,
      [<a href="https://arxiv.org/abs/2406.18522">PDF</a>][<a href="https://github.com/PKU-YuanGroup/ChronoMagic-Bench">Code</a>][<a href="https://pku-yuangroup.github.io/ChronoMagic-Bench/">Project page</a>] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PKU-YuanGroup/ChronoMagic-Bench?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/PKU-YuanGroup/ChronoMagic-Bench"></a> 
    </p>
    <p>
      [5] Fengyang Xiao, Sujie Hu, Yuqi Shen, Chengyu Fang, <strong>Jinfa Huang</strong>, Chunming He, Longxiang Tang, Ziyun Yang, Xiu Li. "A Survey of Camouflaged Object Detection and Beyond", <strong>CAAI 2024</strong>,
      [<a href="https://arxiv.org/abs/2408.14562">PDF</a>][<a href="https://github.com/ChunmingHe/awesome-concealed-object-segmentation">Github</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ChunmingHe/awesome-concealed-object-segmentation?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/ChunmingHe/awesome-concealed-object-segmentation"></a> 
    </p>
    <p>
      [6] Hanjia Lyu*, <strong>Jinfa Huang*</strong>, Daoan Zhang*, Yongsheng Yu*, Xinyi Mou*, Jinsheng Pan, Zhengyuan Yang, Zhongyu Wei, Jiebo Luo. "GPT-4V (ision) as a Social Media Analysis Engine", <strong>ACM Transactions on Intelligence Systems and Technology (TIST) 2024</strong>,
      [<a href="https://arxiv.org/abs/2311.07547">PDF</a>][<a href="https://github.com/VIStA-H/GPT-4V_Social_Media">Code</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/VIStA-H/GPT-4V_Social_Media?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/VIStA-H/GPT-4V_Social_Media"></a> 
    </p>  
    <h3>2023</h3>
    <p>
      [1] Peng Jin, <strong>Jinfa Huang</strong>, Pengfei Xiong, Shangxuan Tian, Chang Liu, Xiangyang Ji, Li Yuan, Jie Chen. "Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning", <strong>CVPR 2023 Highlight</strong>,
      [<a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jin_Video-Text_As_Game_Players_Hierarchical_Banzhaf_Interaction_for_Cross-Modal_Representation_CVPR_2023_paper.html">PDF</a>][<a href="https://github.com/jpthu17/HBI">Code</a>][<a href="https://jpthu17.github.io/HBI/">Project page</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/jpthu17/HBI?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/jpthu17/HBI"></a> 
    </p>
    <p>
      [2] Jingyi Wang, <strong>Jinfa Huang</strong>, Can Zhang, Zhidong Deng. "Cross-Modality Time-Variant Relation Learning for Generating Dynamic Scene Graphs", <strong>ICRA 2023</strong>,
      [<a href="https://ieeexplore.ieee.org/abstract/document/10161478">PDF</a>][<a href="https://github.com/qncsn2016/TR2">Code</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/qncsn2016/TR2?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/qncsn2016/TR2"></a> 
    </p>
    <p>
      [3] Peng Jin, Hao Li, Zesen Cheng, <strong>Jinfa Huang</strong>, Zhennan Wang, Li Yuan, Chang Liu, Jie Chen. "Text-Video Retrieval with Disentangled Conceptualization and Set-to-Set Alignment", <strong>IJCAI 2023</strong>,
      [<a href="https://arxiv.org/abs/2305.12218">PDF</a>][<a href="https://github.com/jpthu17/DiCoSA">Code</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/jpthu17/DiCoSA?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/jpthu17/DiCoSA"></a> 
    </p>
    <p>
      [4] Hao Li, <strong>Jinfa Huang</strong>, Peng Jin, Guoli Song, Qi Wu, Jie Chen. "Weakly-Supervised 3D Spatial Reasoning for Text-Based Visual Question Answering", <strong>TIP 2023</strong>,
      [<a href="https://ieeexplore.ieee.org/abstract/document/10141570">PDF</a>]
    </p>
    <p>
      [5] Jingyi Wang, Can Zhang, <strong>Jinfa Huang</strong>, Botao Ren, Zhidong Deng. "Improving Scene Graph Generation with Superpixel-Based Interaction Learning", <strong>ACMMM 2023</strong>,
      [<a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611889">PDF</a>]
    </p>
    <h3> 2022 and Earlier </h3>
    <p>
      [1] Peng Jin*, <strong>Jinfa Huang*</strong>,  Fenglin Liu, Xian Wu, Shen Ge, Guoli Song, David Clifton, Jie Chen. "Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations", <strong>NeurIPS 2022 Spotlight</strong>,
      [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/c355566ce402de341c3320cf69a10750-Abstract-Conference.html">PDF</a>][<a href="https://github.com/jpthu17/EMCL">Code</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/jpthu17/EMCL?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/jpthu17/EMCL"></a> 
    </p>
    <p>
      [2] Yingmei Guo, <strong>Jinfa Huang</strong>, Yanlong Dong, Mingxing Xu. "Guoym at SemEval-2020 task 8: Ensemble-based Classification of Visuo-lingual Metaphor in Memes", <strong>SemEval-2020</strong>,
      [<a href="https://aclanthology.org/2020.semeval-1.148/">PDF</a>]
    </p>
    <p>
      [3] Yanan Wang, Jianming Wu, <strong>Jinfa Huang</strong>, Gen Hattori, Yasuhiro Takishima, Shinya Wada, Rui Kimura, Jie Chen, Satoshi Kurihara. "LDNN: Linguistic Knowledge Injectable Deep Neural Network for Group Cohesiveness Understanding", <strong>ICMI 2020</strong>,
      [<a href="https://dl.acm.org/doi/abs/10.1145/3382507.3418830">PDF</a>][<a href="https://github.com/yanan1989/Additional-EmotiW-dataset">Code</a>]  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yanan1989/Additional-EmotiW-dataset?logoColor=%23C8A2C8&color=%23DCC6E0"> <a href="https://github.com/yanan1989/Additional-EmotiW-dataset"></a> 
    </p>
  </td>
</tr>
</table> -->

<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Selected Honors & Scholarships</heading>
          <div style="line-height:25px">
          <p>
      <li> <stronghuge> OpenAI Researcher Access Program<stronghuge>, OpenAI&nbsp;  2025<br/>
      <li> <stronghuge>Peking University Excellent Graduation Thesis<stronghuge> (Top 10%), PKU&nbsp;  2023<br/>
		  <li> <stronghuge>Outstanding Graduate of University of Electronic Science and Technology of China (UESTC)</stronghuge>,&nbsp; 2020<br/>
      <li> <stronghuge>Selected entrant for Google Machine Learning Winter Camp 2019</stronghuge> (100 people worldwide),&nbsp; 2019<br/> 
      <li> <stronghuge>National Inspirational Scholarship</stronghuge>,&nbsp; 2018<br/>
      <li> <stronghuge>China Collegiate Programming Contest (ACM-CCPC), Jilin, Bronze</stronghuge>,&nbsp; 2018 <br/>
          </p>
          </div>
        </td>
      </tr>
</table>

<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Talk</heading>
          <div style="line-height:25px">
          <p>
          <li> "Can Video Generation Models as World Simulators?“, 3D视觉工坊, 2025.01, [<a href="https://mp.weixin.qq.com/s/-5lmd0ZRw1IKHFgom_sbzg">Live</a>] 
		<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=113802641150520&bvid=BV1pHc7e1E2u&cid=27792247451&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>
		  <br/>
          </p>
          </div>
        </td>
      </tr>
</table>



<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Teaching</heading>
          <p>
            <li> <stronghuge><strong>Teaching Assistant, CSC 240/440 Data Mining</strong>, Prof. <a href="https://www.cs.rochester.edu/u/pawlicki/">Thaddeus E. Pawlicki</a>, University of Rochester, 2025 Spring</stronghuge> <br/>
            <li> <stronghuge><strong>Teaching Assistant, CSC 240/440 Data Mining</strong>, Prof. <a href="https://www.cs.rochester.edu/people/faculty/polak_monika/index.html">Monika Polak</a>, University of Rochester, 2024 Fall</stronghuge> <br/>
          </p> 
      </td>
      </tr>
</table>

<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Personal Interests</heading>
          <p>
          <stronghuge><strong>Anime</strong>: As a pastime in my spare time, I watched a lot of Japanese anime about love, sports, and sci-fi. </stronghuge>
          </p>
          <p>
          <literature><strong>Literature</strong>: My favorite writer is <a href="https://en.wikipedia.org/wiki/Wang_Xiaobo">Xiaobo Wang</a>, the wisdom of his life inspires me. My favorite philosopher is <a href="https://en.wikipedia.org/wiki/Friedrich_Nietzsche">Friedrich Wilhelm Nietzsche</a>, and I am grateful that his philosophy has accompanied me through many difficult times in my life. </stronghuge>
          </p>     
      </td>
      </tr>
</table>

<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="40%">
          <img src='image/rushrushrush.jpg', width="400" height="200">
        </td>
        <td width="100%" valign="middle">
          <heading>Academic Service</heading>
          <div style="line-height:25px">
          <p>
		  <li> <stronghuge>PC Member:</stronghuge> &nbsp; CVPR'23/24/25, NeurIPS'22/23/25, ICLR'23/24/25, ICCV'23/25, ACM MM'24/25, ECCV'24, AAAI'25, COLM'25, ACL'25, CVEU 2025 Workshop (CVPR'25)<br/>
		  <li> <stronghuge>Journal Reviewer:</stronghuge> &nbsp; IEEE TCSVT, IEEE TPAMI, NEJM AI <br/>  
          </p>
          </div>
        </td>
      </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td>
      <br>
      <div id="clustrmaps-container" style="width: 400px; height: 300px; margin: 0 auto;">
        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=g8JfCuglsVHk6L-tAQR2jVnplj3XNoA-rUSHfEZOf8M&cl=ffffff&w=a"></script>
      </div>
      <!-- <p align="left"><font size="2">
        <a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Finfaaa.github.io&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"/></a> -->
      <p align="left"><font size="2">
      My hometown is Guangdong, you can call me Cantonese name: Gamfaat Wong. <br>
      Last updated on March, 2025. 
      <p align="middle"><font size="2"></font>      
      This awesome template is inspired from this <a href="https://people.eecs.berkeley.edu/~barron/">good man.</a>
      </tbody></table>

</body>
</html>
