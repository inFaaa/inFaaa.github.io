<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Tan Wang</title>
  
  <meta name="author" content="Tan Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="image/favicon_ntu.ico">
</head>

  <body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Tan Wang &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		  <img style="vertical-align:middle" src='image/wangtan_name.gif' height='50px' width='WIDTHpx'>
		  </name>
        </p>
		<p>Currently, Tan Wang is a third-year Ph.D. student at <a href="https://mreallab.github.io/">MReaL Lab</a> of Nanyang Technological University (NTU), supervised by <a href="https://personal.ntu.edu.sg/hanwangzhang/">Prof. Zhang Hanwang</a>. He also works closely with <a href="https://qianrusun.com/">Prof. Qianru Sun</a> from SMU.
		His research interests include but not limit to Visual Reasoning, Causal Inference and Vision & Language.
		</p>
		Before that, He obtained the honoured bachelor degree in <a href="http://www.sice.uestc.edu.cn/">Department of EIE</a> from <a href="https://en.uestc.edu.cn/">University of Electronic Science and Technology of China (UESTC)</a> in 2020. 
		He was a research assistant at <a href="http://cfm.uestc.edu.cn">Center for Future Media </a>, supervised by  Prof. <a href="https://interxuxing.github.io/">Xing Xu</a> and Prof. <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>. 
		He also had a close research collaboration with Prof. <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/multimedia-computing/people/alan-hanjalic/">Alan Hanjalic</a> at TU Delft.
        </p>


        <p align=center>
          <a href="mailto:wangt97[at]hotmail.com">Email</a> &nbsp/&nbsp
          <a href="image/cv_wangtan.pdf">CV</a> &nbsp/&nbsp
		  <a href="https://scholar.google.com.hk/citations?user=wFduC9EAAAAJ&hl=en">Google Scholar</a>  &nbsp/&nbsp
          <a href="https://github.com/Wangt-CN">Github</a> 

        </p>

        </td>
        <td width="33%">
        <img src="image/wangtan3.jpg" width="210">
        </td>
      </tr>
      </table>


<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>News</heading>
          <p>
		  <li> <strongsmall>[2022/08]</strongsmall> &nbsp;&nbsp;<smalll>Received 2022 <a href="http://www.premiasg.org/">PREMIA</a> Best Student Paper Awards (The Gold Award)!</smalll><br/>
		  <li> <strongsmall>[2022/07]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by ECCV 2022.</smalll><br/>
		  <li> <strongsmall>[2022/06]</strongsmall> &nbsp;&nbsp;<smalll>Start summer research internship at Microsoft@Seattle for Vision & Language Pretrained Models.</smalll><br/>
		  <li> <strongsmall>[2022/04]</strongsmall> &nbsp;&nbsp;<smalll>We host the <a href="https://nicochallenge.com/">NICO Challenge 2022</a> for real-world OOD (Out-of-Distribution) generalization problem. Stay tuned!</smalll><br/>
		  <li> <strongsmall>[2022/03]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by CVPR 2022.</smalll><br/>
		  <li> <strongsmall>[2021/10]</strongsmall> &nbsp;&nbsp;<smalll><a href="https://vipriors.github.io/challenges/#:~:text=Jury%20prize%3A-,Tan%20Wang%2C%20Wanqi%20Yin%2C%20Jiaxin%20Qi%2C%20Jin%20Liu%2C%20Jayashree%20Karlekar%2C%20Hanwang%20Zhang.%20Nanyang%20Technological%20University%20%26%20Panasonic%20R%26D%20Center%20Singapore.,-Object%20detection">Jury Prize</a> in ICCV 2021 VIPriors Challenge.</smalll><br/>
		  <li> <strongsmall>[2021/09]</strongsmall> &nbsp;&nbsp;<smalll>1 paper (Spotlight) is accepted by NeurIPS 2021.</smalll><br/>
		  <li> <strongsmall>[2021/07]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by ICCV 2021.</smalll><br/>
		  <li> <strongsmall>[2021/03]</strongsmall> &nbsp;&nbsp;<smalll>1 paper on ZSL/OSR is accepted by CVPR 2021.</smalll><br/>
		  <li> <strongsmall>[2020/04]</strongsmall> &nbsp;&nbsp;<smalll>2 Journal papers are accepted by TNNLS 2020.</smalll><br/>
		  <li> <strongsmall>[2020/02]</strongsmall> &nbsp;&nbsp;<smalll>1 paper with Prof. Hanwang Zhang is accepted by CVPR 2020.</smalll><br/>
          <li> <strongsmall>[2019/07]</strongsmall> &nbsp;&nbsp;<smalll>1 paper with Prof. Alan Hanjalic is accepted by ACM MM 2019 Oral.</smalll><br/>
          
          </p>
        </td>
      </tr>
</table>




      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Education</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <img src='image/uestc_icon.jpg' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>University of Electronic Science and Technology of China (UESTC), China</stronghuge><br />
          Honours Degree in Electronic Information Engineering &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2016 - Jun. 2020 <br />
          GPA: <strong>92.98</strong>/100, &nbsp;&nbsp;Ranking: <strong>2/284</strong> (Overall) or <strong>1/415</strong> (first 2 years)<br />
          Supervisors: Prof. <a href="https://interxuxing.github.io/">Xing Xu</a> and Prof. <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>.  &nbsp;&nbsp; Collaborated with Prof. <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/multimedia-computing/people/alan-hanjalic/">Alan Hanjalic</a>
          </p>
        </td>
      </tr>


        <tr>
          <td width="10%">
            <img src='image/chiba_icon.png' width="105">
          </td>

          <td width="90%" valign="middle">
          <p>
          <stronghuge>Chiba University, Japan</stronghuge><br />
          Exchange Program &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull;  Aug. 2017 <br />
          <a href="http://www.jst.go.jp/crcc/ssc/">Sakura Science Club Scholarship</a> awardee. Funded by Japan Science and Technology Agency <a href="http://www.jst.go.jp/EN/index.html">(JST)</a>.
          
          </p>
        </td>
      </tr>
	  
	  
	    <tr>
          <td width="10%">
            <img src='image/ntu_icon.jpg' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>Nanyang Technological University (NTU), Singapore</stronghuge><br />
          Second-year Ph.D. in <a href="https://mreallab.github.io/">MreaL Lab</a>, School of Computer Science and Engineering &nbsp;&nbsp;&nbsp;&nbsp; &bull; Aug. 2020 - Present <br />
          Supervisor: Prof. <a href="https://personal.ntu.edu.sg/hanwangzhang/">Zhang Hanwang</a>
          </p>
        </td>
      </tr>
	  
      </table>





<p></p><p></p><p></p><p></p><p></p>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research Experience</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <a href="http://cfm.uestc.edu.cn/">
            <img src='image/cfm_icon4.png' width="100">
            </a>
          </td>

          <td width="80%" valign="middle">
          <p>
          <stronghuge>Center For Future Media, UESTC</stronghuge><br />
          <huge><em>Research  Assistant</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Mar. 2018 - Jun. 2020 <br />
          Advisors: &nbsp; Prof. <a href="https://interxuxing.github.io/">Xing Xu</a> and Prof. <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>.  &nbsp;&nbsp;Collaborated with  Prof. <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/multimedia-computing/people/alan-hanjalic/">Alan Hanjalic</a><br/><!--
          <li> Proposed several novel methods for cross-modal retrieval which achieves the state-of-the-art performance on image-text matching.<br/>
          <li> Combined the GCN with Visual Question Generation Task and further boost the performance on an unexplored challenging task zero-shot VQA. <br/>
          <li> Complete 3 works and make the submission.-->
          </p>
        </td>
      </tr>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <a href="https://mreallab.github.io/people.html">
            <img src='image/mreal_icon.png' width="100">
          </a>
          </td>

          <td width="80%" valign="middle">
          <p>
          <stronghuge>MReal Lab, NTU</stronghuge><br />
          <huge><em>Research  Assistant</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; July. 2019 - Aug. 2020 <br />
          Advisors: &nbsp; Prof. <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang </a><!--
          <li> Proposed several novel methods for cross-modal retrieval which achieves the state-of-the-art performance on image-text matching.<br/>
          <li> Comnined the GCN with Visual Question Generation Task and further boost the performance on an unexplored challenging task zero-shot VQA. <br/>
          <li> Complete 3 works and make the submission.-->
          </p>
        </td>
      </tr>
	  
	  
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <a href="https://www.microsoft.com/en-us/research/">
            <img src='image/microsoft_icon3.jpg' width="100">
          </a>
          </td>

          <td width="80%" valign="middle">
          <p>
          <stronghuge>Micorsoft Research, Redmond WA (Remote)</stronghuge><br />
          <huge><em>Research Intern</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Jun. 2022 - Sep. 2022 <br />
          Advisors: &nbsp; <a href="https://sites.google.com/site/kevinlin311tw/me">Kevin Lin</a>, <a href="https://scholar.google.com/citations?user=cDcWXuIAAAAJ&hl=zh-CN">Lijuan Wang</a>, <a href="https://scholar.google.com/citations?user=bkALdvsAAAAJ&hl=en">Zicheng Liu </a>
          </p>
        </td>
      </tr>
  


<p></p><p></p>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publication <a href="https://scholar.google.com/citations?user=wFduC9EAAAAJ&hl=en" style="font-size:22px;">[Google Scholar]</a></heading>
        </td>
      </tr>
      </table>



<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/eqinv/eqinv_wide.png'  width="200" height="150">
      </td>
      <td valign="top" width="75%">
	 <strong>Equivariance and Invariance Inductive Bias for Learning from Insufficient Data</strong><br>
	 <strong>Tan Wang</strong>,
	 <a href="https://qianrusun.com/">Qianru Sun</a>,
	 <a>Sugiri Pranata</a>,
 	 <a>Karlekar Jayashree</a>,    
	 <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a><br>  
        <em>European Conference on Computer Vision, <strong>ECCV 2022</strong></em><br>
		<em><strong><font color="#a82e2e">(Final Rating: 122)</font></strong></em> <br>
		
		<a href="https://arxiv.org/abs/2207.12258">[Paperlink]</a>, <a href="https://github.com/Wangt-CN/EqInv">[Code]</a><br>
        <em>Area: Efficient Learning, Visual Inductive Bias; OOD Generalization</em> <br>
        <p></p>
		<p>We show why insufficient data renders the model more easily biased to the limited training environment, and propose to impose two "good" inductive biases: equivariance and invariance for robust feature learning.</p>
      </td>
    </tr>
   </table>
   
   
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/recam/recam.png'  width="200" height="150">
      </td>
      <td valign="top" width="75%">
	 <strong>Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation</strong><br>
	 <a>Zhaozheng Chen</a>,
	 <strong>Tan Wang</strong>,
	 <a>Xiongwei Wu</a>,
 	 <a>Xian-Sheng Hua</a>,    
	 <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a>,
     <a href="https://qianrusun.com/">Qianru Sun</a><br>
        <em>IEEE International Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2022</strong></em><br>
		
		<a href="https://arxiv.org/abs/2203.00962">[Paperlink]</a>, <a href="https://github.com/zhaozhengChen/ReCAM">[Code]</a><br>
        <em>Area: Weakly-Supervised Semantic Segmentation, CAM</em> <br>
        <p></p>
		<p>We introduce an embarrassingly simple yet surprisingly effective method: Reactivating the converged CAM with Binary Cross Entropy loss (BCE) by using softmax cross-entropy loss (SCE), dubbed ReCAM.</p>
      </td>
    </tr>
   </table>
   
   

 <!-- , bgcolor="#ffffeb"> -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
      <td width="15%">
        <img src='project/ip-irm/framework_github.png'  width="200" height="150">
      </td>
      <td valign="top" width="75%">
	 <strong>Self-Supervised Learning Disentangled Group Representation as Feature</strong><br>
	 <strong>Tan Wang</strong>,
	 <a>Zhongqi Yue</a>,
 	 <a>Jianqiang Huang</a>,    
     <a href="https://qianrusun.com/">Qianru Sun</a>,
     <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a><br>
        <em>Conference and Workshop on Neural Information Processing Systems, <strong>NeurIPS 2021</strong></em><br>
		<em><strong><font color="#a82e2e">(Spotlight Presentation, Top 3%); (2022 PREMIA Best Student Paper)</font></strong></em> <br>
		
		<a href="https://arxiv.org/abs/2110.15255">[Paperlink]</a>, 
		<a href="https://github.com/Wangt-CN/IP-IRM">[Code]</a>, <a href="https://drive.google.com/file/d/1ETPfpmttHwzuTs98LhAQMKObVJOhUq2U/view?usp=sharing">[Poster]</a>, <a href="https://drive.google.com/file/d/1LUSnsPFMyEGc4S0BPc6ZbmIhC_UMgr-s/view?usp=sharing">[Slides]</a>, <a href="https://zhuanlan.zhihu.com/p/483952577">[知乎]</a><br>
		
        <em>Area: Self-supervised Representation Learning, Group Theory, Invariant Risk Minimization</em> <br>
        <p></p>
		<p>We presented an unsupervised disentangled representation learning method called IP-IRM, based on Self-Supervised Learning (SSL). IP-IRM 
		iteratively partitions the dataset into semantic-related subsets, and learns a representation invariant across the subsets using SSL with an IRM loss.</p>
      </td>
    </tr>
   </table>
   
   
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/caam/framework_github.png'  width="200" height="110">
      </td>
      <td valign="top" width="75%">
	 <strong>Causal Attention for Unbiased Visual Recognition</strong><br>
     <strong>Tan Wang</strong>,
	 <a href="https://scholar.google.com/citations?user=QeSoG3sAAAAJ&hl">Chang Zhou</a>,
	 <a href="https://qianrusun.com/">Qianru Sun</a>,
     <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a>,

	  &nbsp <br>
        <em>IEEE International Conference on Computer Vision, <strong>ICCV 2021</strong></em><br>
		<a href="https://arxiv.org/abs/2108.08782">[Paperlink]</a>, 
		<a href="https://github.com/Wangt-CN/CaaM">[Code]</a>, <a href="https://drive.google.com/file/d/1BpT6xPYDKHArqZxthL_98de92mxDkaBd/view?usp=sharing">[Poster]</a>, <a href="https://drive.google.com/file/d/1IZf_DmGX4S06RCVoxLLnR1yLcqvMnIYA/view?usp=sharing">[Slides]</a></em><br>
        <em>Area: Invariant Risk Minimization, OOD Generalization</em> <br>
        <p></p>
		<p>We propose a causal attention module (CaaM) that self-annotates the confounders in unsupervised fashion. In particular, multiple CaaMs can be stacked and integrated in conventional attention CNN and self-attention Vision Transformer.</p>
      </td>
    </tr>
   </table>
   
   
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/gcm_cf/framework_github.png'  width="200" height="110">
      </td>
      <td valign="top" width="75%">
	 <strong>Counterfactual Zero-Shot and Open-Set Visual Recognition</strong><br>
	 <a>Zhongqi Yue*</a>,
     <strong>Tan Wang*</strong>,
     <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a>,
     <a href="https://qianrusun.com/">Qianru Sun</a>,
	 <a href="https://scholar.google.com/citations?user=6G-l4o0AAAAJ&hl=en">Xian-sheng Hua</a> &nbsp (* equal contribution)<br>
        <em>IEEE International Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2021</strong></em><br>
		<a href="https://arxiv.org/abs/2103.00887">[Paperlink]</a>, 
		<a href="https://github.com/yue-zhongqi/gcm-cf">[Code]</a>, <a href="https://arxiv.org/abs/2103.00887">[知乎]</a><br>
        <em>Area: Counterfacual, Zero-shot Learning, Open-set Recognition</em> <br>
        <p></p>
		<p>We presented a novel counterfactual framework "Generative Causal Model" for Zero-Shot Learning (ZSL) and Open-Set Recognition (OSR) to provide a theoretical ground for balancing and improving the seen/unseen classification imbalance.</p>
      </td>
    </tr>
   </table>
   
   

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
      <td width="15%">
        <img src='project/vc-rcnn/framework_github.png'  width="200" height="110">
      </td>
      <td valign="top" width="75%">
	 <strong>Visual Commonsense R-CNN</strong><br>
       <strong>Tan Wang</strong>,
    <a>Jianqiang Huang</a>,
    <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a>,
      <a href="https://qianrusun.com/">Qianru Sun</a> <br>
        <em>IEEE International Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2020</strong></em><br>
		<a href="https://arxiv.org/abs/2002.12204">[Paperlink]</a>, 
		<a href="https://github.com/Wangt-CN/VC-R-CNN">[Code]</a>, <a href="https://zhuanlan.zhihu.com/p/111306353">[知乎]</a></em><br>
        <em>Area: Visual and Language, Causal Reasoning, Self-supervised Learning</em> <br>
        <p></p>
		<p>In this paper, we present a novel un-/self-supervised feature representation learning method, Visual Commonsense Region-based Convolutional Neural Network (VC R-CNN), to serve as an improved visual region encoder for Vision & Language high-level tasks. </p>
      </td>
    </tr>
   </table>


 <!-- , bgcolor="#ffffeb"> -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/vc-rcnn/VC_CVPRW.png'  width="200" height="110">
      </td>
      <td valign="top" width="75%">
	 <strong>Visual Commonsense Representation Learning via Causal Inference (Abstact Version of VC R-CNN)</strong><br>
       <strong>Tan Wang</strong>,
    <a>Jianqiang Huang</a>,
    <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a>,
      <a href="https://qianrusun.com/">Qianru Sun</a> <br>
        <em>IEEE International Conference on Computer Vision and Pattern Recognition MVM Workshop, <strong>CVPRW 2020</strong></em><br>
		<em><strong><font color="#a82e2e">(Oral Presentation)</font></strong></em> <br>
		<a href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w26/Wang_Visual_Commonsense_Representation_Learning_via_Causal_Inference_CVPRW_2020_paper.html">[Paperlink]</a>, <a href="https://github.com/Wangt-CN/VC-R-CNN">[Code]</a>, <a href="https://zhuanlan.zhihu.com/p/111306353">[知乎]</a><br>
        
		<em>Area: Visual and Language, Causal Reasoning, Self-supervised Learning</em> <br>
      </td>
    </tr>
   </table>



    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
      <td width="15%">
        <img src='image/mtfn.png'  width="200" height="130">
      </td>
      <td valign="top" width="75%">
	 <strong>Matching Images and Text with Multi-modal Tensor Fusion and Re-ranking</strong><br>
       <strong>Tan Wang</strong>,
    <a href="https://interxuxing.github.io/">Xing Xu</a>,
    <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>,
      <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/multimedia-computing/people/alan-hanjalic/">Alan Hanjalic</a>,
      <a href="http://cfm.uestc.edu.cn/~shenht/">Heng Tao Shen</a> <br>
	  
        <em>ACM International Conference on Multimedia, <strong>MM 2019</strong></em><br>
		<em><strong><font color="#a82e2e">(Oral Presentation, 4.96% acceptance rate)</font></strong></em> <br>
		<a href="https://arxiv.org/abs/1908.04011">[Paperlink]</a>, <a href="https://github.com/Wangt-CN/MTFN-RR-PyTorch-Code">[Code]</a></em><br>
        
        <em>Area: Visual and Language, Image-text matching</em> <br>
        <p></p>
        <p>In this paper, we propose a novel framework for image-text matching that achieves remarkable matching performance with acceptable model complexity and much less time consuming. </p>
      </td>
    </tr>
   </table>




    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/CASC1.png'  width="200" height="130">
      </td>
      <td valign="top" width="75%">

 
	 
      <strong>Cross-Modal Attention with Semantic Consistence for Image-Text Matching</strong><br>
	  <a href="https://interxuxing.github.io/">Xing Xu*</a>,
      <strong>Tan Wang*</strong>,
	  <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>,
	  Lin Zuo,
      <a href="http://cfm.uestc.edu.cn/~fshen/">Fumin Shen</a>,
      <a href="http://cfm.uestc.edu.cn/~shenht/">Heng Tao Shen</a> &nbsp (* equal contribution) <br>
	      <em>IEEE Transactions on Neural Networks and learning systems, <strong>TNNLS 2020</strong> </em> <br>
          <em>Area: Visual and Language, Image-text matching</em> <br>
        <p></p>
        <p>In this paper, we propose a novel hybrid matching approach named Cross-modal Attention with Semantic Consistence (CASC) for image-text matching, which is a joint framework that performs cross-modal attention for local alignment and multi-label prediction for global semantic consistence.</p>
        <!-- <p . </p> -->
      </td>
    </tr>
   </table>




    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/radial-gcn.png'  width="200" height="130">
      </td>
      <td valign="top" width="75%">
	  <strong>Radial Graph Convolutional Network for Visual Question Generation</strong><br>     
    <a href="https://interxuxing.github.io/">Xing Xu*</a>,
	<strong>Tan Wang*</strong>,
    <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>,
      <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/multimedia-computing/people/alan-hanjalic/">Alan Hanjalic</a>,
      <a href="http://cfm.uestc.edu.cn/~shenht/">Heng Tao Shen</a> &nbsp (* equal contribution) <br>
      <em>IEEE Transactions on Neural Networks and learning systems, <strong>TNNLS 2020</strong> </em> <br>
      <em>Area: Visual and Language, Image-text matching</em> <br>
        <p></p>
        <p>We propose an innovative answer-centric approach termed Radial Graph Convolutional Network (Radial-GCN) to focus on the relevant image regions only to reduce the complexity on VQG task.</p>
      </td>
    </tr>
   </table>



<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Academic Service</heading>
          <div style="line-height:25px">
          <p>
		  <li> <stronghuge>Co-organizer:</stronghuge> &nbsp; <a href="https://nicochallenge.com/">NICO Challenge 2022</a> (ECCV'22 Workshop)<br/>
		  <li> <stronghuge>PC Member:</stronghuge> &nbsp; CVPR'22, ECCV'22, AAAI'23<br/>
		  <li> <stronghuge>Journal Reviewer:</stronghuge> &nbsp; IEEE TNNLS, ACM ToMM<br/>
          </p>
          </div>
        </td>
      </tr>
</table>




<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Talk</heading>
          <div style="line-height:25px">
          <p>
		  <li> "Self-Supervised Learning Disentangled Group Representation as Feature", PREMIA AGM 2022, 2022.08<br/>
		  <li> "Towards Out-of-Distribution Generalization in Computer Vision", National University of Singapore (NUS), 2022.04<br/>
		  <li> "Disentangled Group Representation Learning and its Potential in Causality", ZhiYuan Community, 2022.01<br/>
		  <li> "Generalization Powered by Invariant Learning", Singapore Management University (SMU), 2021.11<br/>
		  <li> "因果推理的应用与发展 (中文)", AI Time, 2021.10 &nbsp<a href="https://www.bilibili.com/video/BV1qP4y1b7us?spm_id_from=333.999.0.0">[Video]</a><br/>
		  <li> "Visual Commonsense R-CNN", National University of Singapore (NUS), 2021.03<br/>
          </p>
          </div>
        </td>
      </tr>
</table>



<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Honors & Scholarships</heading>
          <div style="line-height:25px">
          <p>
		  <li> <stronghuge>2022 PREMIA Best Student Paper Award (The Gold Award)</stronghuge>,&nbsp; 2022<br/>
		  <li> <stronghuge>Jury Prize in ICCV 2021 VIPriors Challenge</stronghuge>,&nbsp; 2021<br/>
		  <li> <stronghuge>NTU Research Scholarship</stronghuge>,&nbsp; 2020<br/>
		  <li> <stronghuge>Outstanding Graduates of Sichuan Province</stronghuge> (Top 1% student),&nbsp; 2020 &nbsp<a href="https://mp.weixin.qq.com/s/GEXwBLLaP3qCM3bZCx1kXw">[Press Coverage]</a><br/>
          <li> <stronghuge>Outstanding Undergraduate Thesis Award</stronghuge> (Top 2% student),&nbsp; 2020<br/>
          <li> <stronghuge>National Scholarship</stronghuge> (Top 2% student),&nbsp; 2017, 2018<br/>
          <li>  <stronghuge>Tang Lixin Sponsored Elite Scholarship</stronghuge> (Only 60 awardees pre year in UESTC),&nbsp; 2017 <br/>
          <li> <stronghuge>Best Freshman Award</stronghuge> (Top 1 student per year in Department),&nbsp; 2016<br/>
          <li> <stronghuge>Honor Student Scholarship</stronghuge> (Top 10 students per year in Department),&nbsp; 2018<br/>
		  <li> <stronghuge>Outstanding Student Scholarship</stronghuge> (Top 10% student),&nbsp; 2017~2019<br/>
          </p>
          </div>
        </td>
      </tr>
</table>



<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Project</heading>
		  <p>
		  <div style="text-align: center;">
		  <img src='project/projects_icon.png'  width="600">
		  </p>
          </div>
        </td>
      </tr>
</table>



<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Leadership Experience</heading>
      </td>
      </tr>
      </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/lecture.jpg'  width="195" height="130">
      </td>
      <td valign="top" width="75%">
        <stronghuge>Lecture Group of EE Department</stronghuge> </br>
        <huge><em>Founder & President</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Oct. 2017 - Sep. 2018 </br>
        <p></p>
        <p>
         <li> Organized academic forum, sharing sessions, Q&A meetings more than 30 times, serving over 1000 students on studying and future planing.<br/>
          <li> The team grows to 30 people and won the Outstanding Student Organisation prize in 2018.<br/>
          </p> 
      </td>
    </tr>
   </table>


       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="dachuang_stop()" onmouseover="dachuang_start()" >
      <td width="26%">
        <div class="one">
                <div class="two" id='dachuang_image'><img src='image/dachuang2.png'  width="195" height="130"></div>
                <img src='image/dachuang1.png'  width="195" height="130">
              </div>
      <script type="text/javascript">
                function dachuang_start() {
                  document.getElementById('dachuang_image').style.opacity = "1";
                }
                function dachuang_stop() {
                  document.getElementById('dachuang_image').style.opacity = "0";
                }
                dachuang_stop()
              </script>
      </td>

      <td valign="top" width="75%">
        <stronghuge>Innovative Entrepreneurship Project of UESTC</stronghuge> </br>
        <huge><em>Team Leader</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Sep. 2017 - Mar. 2018 </br>
        <p></p>
        <p>
         <li> This project focus on the pedestrian detection in low-light condition with excellent conclusion. We combine the recent pedestrian detection models with the low-light image enhancement algorithm based on Laplace operator.<br/>
         <li> Responsible for the code implementation and project promotion.<br/>
          </p> 
      </td>
    </tr>
   </table>
          




<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Personal Interests</heading>
          <p>
          <stronghuge>DOTA1</stronghuge>: My first and most playing PC game which accompanied me in my whole middle and high school. And I got about 1350 score  on the '11' Battle Platform Ladder Tournament. :)
          </p>
          <p>
          <stronghuge>Running</stronghuge>: During my college, I offen run a long distance for the pleasure releasing. And I have participated in the Chengdu Shuangyi Marathon in 2018.
          </p>
      </td>
      </tr>





   <p></p><p></p><p></p><p></p><p></p>
   <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=250&t=tt&d=85Rlf3OqLYVhTE6hGEcHnAsDJl6O0EsUp326ZMpLzCI"></script>
   
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<br>
				<p align="left"><font size="2">
				Last updated on Aug, 2022
				<p align="middle"><font size="2">
				This awesome template borrowed from <a href="https://people.eecs.berkeley.edu/~barron/">this guy</a>~
				</tbody></table>
   

</body>
</html>
